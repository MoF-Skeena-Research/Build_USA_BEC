---
title: "Model USA Biogeoclimatic Units using Machine Learning"
author: "William H MacKenzie and Kiri Daust"
date: "17/10/2019"
output: 
   html_document:
  code_folding: hide
  #theme: flatly
---
### Script to create RF model from USA training points and predict + map US zones/ subzones (predicted within each zone)
1. Import training data
2. Reduce variable list with caret or PCA all variable
3. Test machine learning model with analysis/assessment splits and CV
4. Report errors rate
5. Build final model
6. Predict 
Translates predictions to a grid point map
Add predictions to hex polygon layer.
Overlay additional plots over map to assign to USA_BGC units


# STEP 1: Prepare dataset for analysis----clear workspace
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
require(reshape)
require(reshape2)
require(parallel)
require(foreach)
require(doParallel)
require(ggplot2)
require(dplyr)
require(ranger)
require (tools)
require(data.table)
require(spatstat)
require(spatialEco)
require(survey)
require(scales)
require (tidyverse)
require(rlang)
require (Rcpp)
require(forcats)
require(purrrlyr)
require(skimr)
require(smotefamily)
require(tictoc)
require(tidymodels)
require(spatialsample)
require(themis)
require(sf)
# require(conflicted)
require(ggtext)
tidymodels_prefer()
#install.packages ("spThin")
conflicted::conflict_prefer(name = "spec", winner = "yardstick")

source("./_functions/AddVars.R")
```


####Predict BGC membership of hex grid
### Use the best model from those above (Biological, nzv and corr removed, Scaled, Outliers Removed)
``` {r Load model}
# load("./outputs/USAv12_tidymodel.Rdata")
load("./outputs/USAv12_w_BCborder_tidymodel.Rdata")
model_vars <- BGCmodel$fit$fit$fit
model_vars <- model_vars$variable.importance
model_vars <- as.data.frame(model_vars) %>% tibble::rownames_to_column()

```

```{r select and predict state}
#load("./outputs/USAv12_62nosmote_VAR_SubZone_ranger.Rdata")
# bring in grid data with climate attributes
region = "MT"
Z1 <- fread(paste0("./inputs/grids/", region, "_400m_HexPts_Normal_1961_1990MSY.csv"), stringsAsFactors = FALSE,  data.table = FALSE)
Z1_loc <-  Z1 %>% dplyr::select(ID1, Latitude, Longitude, Elevation)
Z1_cna <- Z1 %>% dplyr::select(ID1,ID2, Latitude, Longitude, Elevation)
fwrite(Z1_cna, paste0("./inputs/grids/", region, "_400m_HexPts.csv"))
Z1 <- addVars(Z1)
#grid.dat <- fread("./inputs/grids/USA_8000m_Grid_Climate_new_Normal_1961_1990MSY.csv", stringsAsFactors = FALSE,  data.table = FALSE)### add in the extra climate variables



#modelvars <- read.csv("./inputs/Final22Var_WNABGCv11.csv")##use model variables from buildrf work
#modelvars <- modelvars %>% filter(x !="BGC")
grid.dat <- Z1 %>% dplyr::select(ID1,model_vars$rowname) #%>%  filter_all(all_vars(. > -9999))

grid.dat$ID1 <- as.character(grid.dat$ID1 )
###Predict
grid.pred <- predict(BGCmodel, grid.dat)

Z1_loc <- Z1_loc[Z1_loc$ID1 %in% grid.dat$ID1,]
Z1.pred <- cbind(Z1_loc, grid.pred$.pred_class) %>% rename(BGC.pred = 'grid.pred$.pred_class')
#Z1.
#grid.dat$Zone <- as.factor(grid.dat$Zone)
Z1.pred$BGC <-  fct_explicit_na(Z1.pred$BGC.pred , na_level = "(None)")
#write.csv(grid.dat, "./outputs/Hexgrid800mPredictedtoSubZone_30vars.csv", row.names = FALSE)
Z1.pred <- dplyr::select(Z1.pred, ID1, BGC, Latitude, Longitude,Elevation )#"Longitude","Latitude", "Elevation", "Zone")]
Z1.pred$ID1 <- as.character(Z1.pred$ID1)
#write.csv(Z1.pred, "./outputs/ID_400mPredictedtoSubZone_Jun7.csv", row.names = FALSE)
```

# Attribute hex grid with subzone/variant call
```{r link to hex polygon layer}
require(lwgeom)
##############link predicted Zones to Polygons and write shape file
tic()
hexpoly <- st_read(dsn = paste0("./inputs/hex_shapes/", region, "_bgc_hex400.gpkg"))#, layer = "USA_bgc_hex_800m")
hexpoly$hex_id <- as.character(hexpoly$hex_id)
hexZone <- left_join(hexpoly, Z1.pred, by = c("hex_id" = "ID1"))# %>% st_transform(3005) %>% st_cast()
temp <- hexZone %>% select(BGC, geom)
temp2 <- st_zm(temp, drop=T, what='ZM') 
##unremark for undissolved layer
#st_write(temp2, dsn = "./outputs/MT_SubZoneMap_400m_undissolved_7Oct2020.gpkg", driver = "GPKG")

###Dissolve 
#hexZone <- st_read(dsn = "./outputs/WA_bgc_hex8000_ungrouped.gpkg")#, layer = "USA_bgc_hex_800m") ## need to read it back in to ensure all type Polygon is consistent
temp3 <- hexZone
temp3$BGC <- droplevels(temp3$BGC)
temp3 <-  st_as_sf(temp3)# 
st_precision(temp3) <- .5 
temp3$BGC <- forcats::fct_explicit_na(temp3$BGC,na_level = "(None)")
temp3 <- temp3[,c("BGC","Elevation","geom")]
t2 <- aggregate(temp3[,-1], by = list(temp3$BGC), do_union = T, FUN = mean) %>% rename(BGC = Group.1)

wna_boundary = st_read("D:/CommonTables/BC_AB_US_Shp/WNA_State_Boundaries.gpkg") %>% st_as_sf() %>% filter(State %in% region) %>%
  st_transform( crs = st_crs(3005)) %>%
  st_buffer(., dist = 0)# %>%
 # as(., "Spatial")

t2 <- st_zm(t2, drop=T, what='ZM') %>% st_transform(crs = st_crs(3005))  %>% st_buffer(0)
t2 <- st_intersection(t2, wna_boundary)
#mapView(t2)
#CRS.102008 <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m no_defs"
#CRS.102218 <- "+proj=aea +lat_1=43 +lat_2=48 +lat_0=34 +lon_0=-120 +x_0=600000 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"


#t3 <- st_transform_proj(t3, CRS.102218)
#st_is_valid(t3)
#t4 <- st_make_valid(t3)## this seems to remove rather than fix problem areas
 st_write(t2, dsn = paste0("./outputs/", region, "_SubZoneMap_hex400_dissolved_19May2021_clipped_BC.gpkg"), driver = "GPKG", delete_dsn = TRUE)
 toc() ##WA takes approx 200s

```
## clean up isolated individual pixels
This process is slow. IS probably better done in QGIS.
In QGIS
Open the export file which is a multipolygon and separate multipart to single parts, (v fast),  add geometric attributes to polygons(v fast), select small polygons by < X area,  select polygons <600 000 = four 400m hexes in size (v fast), eliminate selected polygons by longest neighbour, re dissolve by BGC with SAGA.
Next Step to Polygonize raster map and eliminate small polygons



```{r clean crumbs}
## Read in map if not already loaded from previous chunk
#region <- "WA"
#t2 <- st_read(dsn = paste0("./outputs/", region, "_SubZoneMap_hex400_dissolved_7Jan2021_clipped.gpkg"))

tic()
t2a <- st_cast(t2, "MULTIPOLYGON") %>% st_cast("POLYGON")
t2a <- t2a %>%
  mutate(Area = st_area(.)) %>%
  mutate(ID = seq_along(BGC))

require (units)
size <- 600000
size <- set_units(size, "m^2")
tSmall <- t2a[t2a$Area <= size,]
t2a$BGC <- as.character(t2a$BGC)
toc()
require(doParallel)
coreNum <- as.numeric(detectCores()-1)
coreNo <- makeCluster(coreNum)
registerDoParallel(coreNo, cores = coreNum)

###loop through each polygon < size, determine intersects, and assign to BGC with most edge touching
###all the built in functions Kiri found only dealt with holes in the middle of polygons
tic()
new <- foreach(i = 1:length(tSmall$ID), .combine = rbind, .packages = c("foreach","sf")) %dopar% {
  ID <- tSmall$ID[i]
  nbrs <- st_intersects(tSmall[i,],t2a)[[1]]
  nbrs <- nbrs[!nbrs %in% ID]
  if(length(nbrs) == 0){return(NULL)}
  lines <- st_intersection(t2a[ID,],t2a[nbrs,])
  lines <- st_cast(lines)
  l.len <- st_length(lines)
  names(l.len) <- lines$BGC.1
  zn <- names(l.len)[l.len == max(l.len)][1]
  newDat <- t2a[ID,]
  newDat$BGC <- zn
  newDat
}

stopCluster(coreNo)
gc()
toc() ###approx 10 minutes

tic()
temp <- t2a[!t2a$ID %in% new$ID,]
t2a <- rbind(temp, new) %>%
  mutate(BGC = as.factor(BGC))
#
# ###now have to combine crumbs with existing large polygons
temp2 <- t2a
st_precision(temp2) <- 2
t3 <- temp2 %>%
  group_by(BGC) %>%
  summarise(geom = sf::st_union(geometry)) %>%
  ungroup()
#
#mapview(t2, zcol = "BGC")
t3 <- st_zm(t3, drop=T, what='ZM')
st_write(t3, dsn = paste0("./outputs/", region, "_BGC_20May2021_eliminated_BC.gpkg"), driver = "GPKG",delete_dsn = TRUE)
toc()
```


```{r smooth}
## smooth polygon boundaries
tic()
#t3_smooth <- smoothr::smooth(t3, method = "densify")
t3_smooth <- smoothr::smooth(t3_smooth, method = "ksmooth", smoothness = 2)#)
#t3_smooth.spline <- smoothr::smooth(t3_smooth, method = "spline")
# holes = .5
# area_thresh <- units::set_units(holes, km^2)
# p_dropped <- smoothr::fill_holes(t3_smooth, threshold = area_thresh)
# t3_smooth <- p_dropped
#st_write(t3_smooth, dsn = paste0("./outputs/", region, "_BGC_19May2021_smoothed_densify-smooth.gpkg"), driver = "GPKG",delete_dsn = TRUE)
st_write(t3_smooth.spline, dsn = paste0("./outputs/", region, "_BGC_19May2021_smoothed.gpkg"), driver = "GPKG",delete_dsn = TRUE)
toc()
```

