---
title: "Predict WNA Biogeoclimatic Units using Machine Learning"
author: "William H MacKenzie and Kiri Daust"
date: "20/03/2020"
output: html_document
---
# Script to build a WNA_BGC map from a machine-learning model applied to a hex raster grid for all of Western North America.

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(rattle)
require(rpart)
require(rpart.plot)
require(plyr)
require(reshape)
require(reshape2)
#require(VSURF)
#require(reports)
#require(rpart.utils)
#require(rfUtilities)
require(parallel)
require(foreach)
require(doParallel)
require(ggplot2)
#require(functional)
require(plot3D)
require(dplyr)
require(tcltk)
require(caret)
#require(randomForest)
require(ranger)
#require (OpenMP)
#require (randomForestSRC)
require (tools)
require(data.table)
require(spatstat)
require(spatialEco)
require(survey)
require(scales)
require(UBL)
#require (mcl3)
require(tidyr)
require (tidyverse)
require(rlang)
require (Rcpp)
require(sf)
require (mapview)
require(forcats)
require(Boruta)
require(purrrlyr)
require(skimr)
require(gbm)
require(vcd)
require(alluvial)
#install.packages ("alluvial")
```

# Read in hexgrid point data with climate data
CSV file with all variable climate data generated by ClimateNA and ClimateBC for the 1960-91 Normal Period.
Several additional climate variables are calculated
```{r input data, echo=FALSE}
##merge together parts of hexgrid that were separately submitted to BC vs NA
# X1 <- fread("D:/CommonTables/HexGrids/WNAv11_HexGrid_800m_Cleaned_BC_Normal_1961_1990MSY.csv",  stringsAsFactors = FALSE,data.table = FALSE)
# X2 <- fread("D:/CommonTables/HexGrids/WNAv11_HexGrid_800m_Cleaned_USA_AB_Normal_1961_1990MSY.csv",  stringsAsFactors = FALSE,data.table = FALSE)
# X3 <- rbind(X1,X2)
#fwrite(X3, "D:/CommonTables/HexGrids/WNAv11_HexGrid_800m_Cleaned_Normal_1961_1990MSY.csv", row.names = FALSE)

# 16 variable set for rF climate modelling - original
# Columns = c("ID1", "ID2",#sometime the second column is ID2
#             "TD",	"Eref",	"AHM",	"Tmax07",	"PPT06","MCMT","PPT_sp","Tmin_at",
#             "PAS", "DD5_sp","bFFP",	"Tmin_sm","Tmax_sp","PPT_at","PPT_wt", "CMD", "CMD_sp" )

load("D:/CommonTables/MachineLearningModels/WNAv11_53VAR_SubZone_ranger.Rdata")
var.imp <- as.data.frame(BGCmodel$variable.importance) %>% rownames_to_column("vars")
var.list <- as.character(var.imp$vars, "ID1")
 Columns = c("ID1", "ID2",var.list)#sometime the second column is ID2
             # "TD",	"Eref",	"AHM",	"MCMT","PPT_sp",
             # "PAS", "DD5_sp","bFFP",	"Tmin_sm", "CMD_sp", "NFFD", "RH_sm",
             # "PPT05","PPT06","PPT07", "PPT08", "PPT09","PPT_at","PPT_wt", "CMD", "CMD07")

X1 <- fread("D:/CommonTables/HexGrids/WNAv11_HexGrid_800m_Cleaned_Normal_1961_1990MSY.csv",  stringsAsFactors = FALSE,data.table = FALSE)

X1$PPT.dormant <- X1$PPT_at + X1$PPT_wt # for calculating spring deficit
X1$CMD.def <- 500 - (X1$PPT.dormant)# start of growing season deficit original value was 400 but 500 seems better
X1$CMD.def [X1$CMD.def < 0] <- 0 #negative values set to zero = no deficit
X1$CMD.total <- X1$CMD.def + X1$CMD
X1$PPT_MJ <- X1$PPT05 + X1$PPT06 # MaY/June precip
X1$PPT_JAS <- X1$PPT07 + X1$PPT08 + X1$PPT09 
X1$CMD.grow <- X1$CMD05 + X1$CMD06 +X1$CMD07 +X1$CMD08 +X1$CMD09# July/Aug/Sept precip
X1$CMDMax <- X1$CMD07
X2 <- X1 %>% select(ID1,ID2,all_of(var.list)) #%>% filter(ID2 == c("CO", "ID", "NV", "UT", "WA", "WY", "CA", "OR", "MT" )) %>% select(-ID2)
X2 <- lapply(X2, as.numeric)
#drops <- c("PPT_at", "PPT_wt", "PPT.dormant", "CMD", "ID2", "PPT05", "PPT06", "PPT07", "PPT08", "PPT09", "CMD.def", "CMD07")
#X1 <- X1[ ,!(names(X1) %in% drops)]
#X1$ID1 <- row.names(X1)
###adding in a 3rd grid  
  #fplot3 = (file.choose())###add in USA
  #Y1 <- fread(fplot3, stringsAsFactors = FALSE, data.table = FALSE)#
  #Y1 <- Y1[ ,!(names(Y1) %in% drops)]
  #Y1 <- Y1[-1]
  #X1 <- rbind(X1,Y1)

fwrite (X2, "D:/CommonTables/HexGrids/WNAv11_HexGrid_800m_Cleaned_w_53Vars.csv", row.names = FALSE)

```



####Predict zone membership of hex grid
### Use the best model from those above (Biological, nzv and corr removed, Scaled, Outliers Removed)
``` {r Build }

load("D:/CommonTables/MachineLearningModels/WNAv11_53VAR_SubZone_ranger.Rdata")
#load("D:/CommonTables/MachineLearningModels/xxBGCv11_AB_USA_16VAR_SubZone_RFmodel.Rdata")
# bring in grid data with climate attributes
Z1 <- fread("D:/CommonTables/HexGrids/WNAv11_HexGrid_800m_Cleaned_w_53Vars.csv", stringsAsFactors = FALSE,  data.table = FALSE)
Z1 <- lapply(Z1, as.numeric)
Z1 <- as.data.frame(Z1)
#grid.dat <- fread("./inputs/grids/USA_8000m_Grid_Climate_new_Normal_1961_1990MSY.csv", stringsAsFactors = FALSE,  data.table = FALSE)### add in the extra climate variables
  #grid.dat <- Z1 %>%  filter_all(all_vars(. > -9999))
###Predict
grid.dat$BGC <- predict(BGCmodel, data = grid.dat[,c(-1)])$predictions
grid.dat$BGC <-  fct_explicit_na(grid.dat$BGC , na_level = "(None)")
#write.csv(grid.dat, "./outputs/Hexgrid800mPredictedtoSubZone_30vars.csv", row.names = FALSE)
grid.BGC <- grid.dat %>% dplyr::select( ID1, BGC )#"Longitude","Latitude", "Elevation", "Zone")]
fwrite(grid.BGC, "./outputs/WNAv11Hexgrid800mPredictedtoSubZone_Mar20.csv", row.names = FALSE)
```

# Attribute hex grid with Zone call
```{r link to hex polygon layer}
##############link predicted Zones to Polygons and write shape file
hexpoly <- st_read(dsn = "D:/CommonTables/HexGrids", layer = "WNAv11_HexPoly_800m_Mar11")
hexZone <- left_join(hexpoly, grid.BGC, by = c("FID"="ID1"))
temp <- select(hexZone, BGC, geometry)

temp2 <- temp
st_precision(temp2) <- 0.5###union polygons within zone
temp2$BGC <-  as.factor(temp2$BGC)
temp2$BGC <- forcats::fct_explicit_na(temp2$BGC,na_level = "(None)")
droplevels(temp2$BGC)
t2 <- temp2 %>%
  group_by(BGC) %>%
  summarise(geometry = sf::st_union(geometry)) %>%
  ungroup()
mapView(t2)
t2 <- st_zm(t2, drop=T, what='ZM')
st_write(t2, dsn = "outputs", layer = "WNAv11_rawSubZoneMap_800m_ranger_30_2", driver = "ESRI Shapefile", update = TRUE)

```
## clean up isolated individual pixels
This process is slow. IS probably better done in QGIS.
In QGIS
Take the multipolygon, separate into polygons (v fast), add geometric attributes (v fast), select polygons < x in size (v fast), eliminate selected polygons merging with smallest neighbour(slow)
Remove crumbs of 3 hex or smaller by longest neibour

``` {r cleanup up crumbs}

###now cleanup and remove crumbs
# t2 <- st_cast(t2, "MULTIPOLYGON") %>% st_cast("POLYGON")
# t2 <- t2 %>%
#   mutate(Area = st_area(.)) %>%
#   mutate(ID = seq_along(Zone))
# unique(t2$Area)
# 
# 
# library(units)
# size <- 2000000
# size <- set_units(size, "m^2")
# tSmall <- t2[t2$Area <= size,]
# t2$Zone <- as.character(t2$Zone)
# 
# require(doParallel)
# coreNum <- as.numeric(detectCores()-1)
# coreNo <- makeCluster(coreNum)
# registerDoParallel(coreNo, cores = coreNum)
# 
# ###loop through each polygon < size, determine intersects, and assign to zone with most edge touching
# ###all the built in functions I found only dealt with holes in the middle of polygons
# new <- foreach(i = 1:length(tSmall$ID), .combine = rbind, .packages = c("foreach","sf")) %dopar% {
#   ID <- tSmall$ID[i]
#   nbrs <- st_intersects(tSmall[i,],t2)[[1]]
#   nbrs <- nbrs[!nbrs %in% ID]
#   if(length(nbrs) == 0){return(NULL)}
#   lines <- st_intersection(t2[ID,],t2[nbrs,])
#   lines <- st_cast(lines)
#   l.len <- st_length(lines)
#   names(l.len) <- lines$Zone.1
#   zn <- names(l.len)[l.len == max(l.len)][1]
#   newDat <- t2[ID,]
#   newDat$Zone <- zn
#   newDat
# }
# 
# stopCluster(coreNo)
# gc()
# temp <- t2[!t2$ID %in% new$ID,]
# t2 <- rbind(temp, new) %>%
#   mutate(Zone = as.factor(Zone))
# 
# ###now have to combine crumbs with existing large polygons
# temp2 <- t2
# st_precision(temp2) <- 0.5
# t2 <- temp2 %>%
#   group_by(Zone) %>%
#   summarise(geometry = sf::st_union(geometry)) %>%
#   ungroup()
# 
# mapview(t2, zcol = "Zone")
# t2 <- st_zm(t2, drop=T, what='ZM')
# st_write(t2, dsn = "outputs", layer = "DeCrumbed_SubZoneMap_800m_ranger30", driver = "ESRI Shapefile", update = TRUE)

```


# Build Subzone models and predict within each Zone
####now build models for each subzone and then predict gridpoints by subzone model
Advice of Tom Hengel is that single model is more mathematically justified

``` {r Build Subzone model and predict}

# # Grid data from previous chunk
# pointDat <- grid.dat
# pointDat <-  pointDat %>% select(Zone, everything()) %>% arrange(Zone)
# 
# ######################################################
# #Training point data from above
# X2 <-  X1.info %>% select(Zone, everything()) %>% arrange(Zone)
# X2$BGC <-  as.factor(X2$BGC)
# Zones <- as.character(unique(X2$Zone))
# Zones <- sort (Zones)
# X2 <- droplevels(X2)
# # X2 <- X2[X2$Zone == "CMA",]
# # Zones <- as.character(unique(X2$Zone))
# # Zones <- sort (Zones)
# SZPred <- foreach(Z = Zones, .combine = rbind) %do% {
#   trainSub <- X2[X2$Zone == Z,]###subset training points to only include selected zone
#    if(length(unique(trainSub$BGC)) >= 2){ ###somtimes there aren't any subzones skip to else
#     trainSub$BGC <- as.factor(trainSub$BGC)
#     trainSub$BGC <- droplevels(trainSub$BGC)
#     trainSub <- removeOutlier(trainSub, alpha = 0.001)
# ###build model for each subzone individually
#     set.seed(123321)
# coreNo <- makePSOCKcluster(detectCores() - 1)
# registerDoParallel(coreNo, cores = detectCores() - 1)
# Cores <- as.numeric(detectCores()-1)
# 
# SZmodel <- train(BGC  ~ ., data = trainSub [-1],
#                      method = "ranger",
#                      trControl = trainControl(method="cv", number = 10, verboseIter = T, classProbs = T, savePredictions = "final"),
#                      #num.trees = 11,
#                      preProcess = c("center", "scale", "YeoJohnson"),#,tuneGrid = tgrid,
#                      importance = "impurity")
# 
#  stopCluster(coreNo)
# gc()
#       # SZmodel <- randomForest(BGC ~ ., data=trainSub [-1], nodesize = 5, do.trace = 10,
#       #                       ntree=101, na.action=na.fail, importance=TRUE, proximity=FALSE)
# 
#    
#     pointSub <- pointDat[pointDat$Zone == Z,] ###subset grid based on previously predicted zones
#     pointSub$BGC<- predict(SZmodel, newdata = pointSub[,-c(1:4),]) ### predict subzones
#     out <- pointSub[,c("ID1", "Latitude","Longitude","Zone", "BGC")]
#     out
#   }else{ ##if only one subzone, plot
#     pointSub <- pointDat[pointDat$Zone == Z,]
#     #pointSub$Zone <- droplevels(pointSub$Zone)
#     pointSub$BGC <- pointSub$Zone
#     out <- pointSub[,c("ID1", "Latitude","Longitude","Zone", "BGC")]
#     out
#   }
#   
# }
# 
# grid.sbz <- dplyr::select(SZPred, ID1, BGC, Zone)#"Longitude","Latitude", "Elevation", "Zone")]
# grid.sbz <- droplevels(grid.sbz)
# table(grid.sbz$BGC)
# grid.dat.pred <- left_join(grid.dat, grid.sbz, by = c("ID1", "Zone"))
# grid.dat.pred <- grid.dat.pred %>% select(ID1, Zone, BGC, everything())
# write.csv(grid.dat, "./outputs/Hexgrid800mPredictedtoSubZone_ranger.csv", row.names = FALSE)
# ```
# # Attribute hexpolygons with subzone predictions 
# ```{r link subzone predictions to hex polygons}
# ###link to hex polygons
# hexZone2 <- left_join(hexZone, grid.sbz, by = "ID1")
# #temp <- select(hexZone, Zone, geometry)
# temp <- hexZone2
# temp$BGC <-  fct_explicit_na(temp$BGC , na_level = "(None)")
# table(temp$BGC)
# 
# temp2 <- temp
# st_precision(temp2) <- 0.5###union polygons within zsubone
# t3 <- temp2 %>%
#   group_by(BGC) %>%
#   summarise(geometry = sf::st_union(geometry)) %>%
#   ungroup()
# 
# mapview(t3)
# t3 <- st_zm(t3, drop=T, what='ZM')
# st_write(t3, dsn = "outputs", layer = "SubzoneRaw800m_ranger_30", driver = "ESRI Shapefile", update = TRUE)


```

## This should probably be done from the spatial file since it will have be simplified with decrumbing

```{r cLHS of trainingpoints for WNA}
#select a subset of training points by BGC from the final surface for full model build
# library(clhs)
# #BGC = "ICHxwz"
# #countZone <- points %>% count(Zone)
# countSubzone <- grid.dat.pred %>% count(BGC)
# 
# rownames(grid.dat.pred) <- grid.dat.pred[,1]
# countSubzone$logn <- log(countSubzone$n, 10)
# countSubzone$rs <- as.integer (rescale(countSubzone$logn, to = c(500, 1200), from = range(countSubzone$logn, na.rm = TRUE, finite = TRUE)))
# countSubzone$sample <- ifelse(countSubzone$rs > countSubzone$n, countSubzone$n, countSubzone$rs )
# write.csv (countSubzone, "./outputs/USA_pts_per_BECLHC2.csv")
# 
# allUnits <- unique(grid.dat.pred$BGC)
# #grid.dat.pred$ID1 <- row.names(grid.dat.pred)
# set.seed(123321)
# coreNo <- makePSOCKcluster(detectCores() - 1)
# registerDoParallel(coreNo, cores = detectCores() - 1)
# Cores <- as.numeric(detectCores()-1)
# 
# BGC = "BGdh_OR"
# LHCtraining <- foreach(BGC = allUnits, .combine = rbind, .packages = c("clhs")) %dopar% {
# temp <- grid.dat.pred[(grid.dat.pred$BGC %in% BGC),]
# temp_names <- temp[1:6]
# Num <- countSubzone$sample[(countSubzone$BGC %in% BGC)]
#   samples <- clhs(temp[,-c(1:6)], 
#                 size = Num,           # Test a range of sample sizes
#                 iter = 10000,        # Arbitrarily large number of iterations of optimization procedure. Default=10,000 but a larger number may be used
#                 progress = TRUE, 
#                 simple = FALSE)
# 
# cLHS <- samples$sampled_data
# cLHS$ID1 <- row.names (cLHS)
# cLHS_Points <- merge (temp[,c(1:5)], cLHS, by = "ID1")
# cLHS_Points
# }
#  stopCluster(coreNo)
#  gc()
# 
# write.csv(LHCtraining, "./outputs/USA_Training_LHC_w_data.csv", row.names = FALSE)
# LHCtraining2 <- LHCtraining [1:3] 
# rownames(grid.dat.raw) <- grid.dat.raw[,1]
# LHCtraining2 <- left_join(LHCtraining2, grid.dat.raw, by = "ID1")
# write.csv(LHCtraining2, "./outputs/USA_LHS_all_dat.csv", row.names = FALSE)
# USA_LHS <- dplyr::select(LHCtraining2, ID1, BGC, Longitude, Latitude, Elevation)
# write.csv(USA_LHS, "./outputs/USA_LHS_for_ClimateNA.csv", row.names = FALSE)
#X1 <- LHCtraining #feed these back into building a new rF model above
```
