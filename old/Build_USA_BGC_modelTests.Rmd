---
title: "Model USA Biogeoclimatic Units using Machine Learning"
author: "William H MacKenzie and Kiri Daust"
date: "17/10/2019"
output: html_document
---
# Script to create RF model from USA training points and predict + map US zones/ subzones (predicted within each zone)
Import training data
Reduce variable list with caret or PCA all variable
Build machine learning model
Test for missing variable space or map error for required additional training points
Loop to build models by zone and then by subzone within zone to predict map
Translates predictions to a grid point map
Add predictions to hex polygon layer.
Overlay additional plots over map to assign to USA_BGC units


# STEP 1: Prepare dataset for analysis----clear workspace
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(rattle)
require(rpart)
require(rpart.plot)
require(plyr)
require(reshape)
require(reshape2)
#require(VSURF)
#require(reports)
#require(rpart.utils)
#require(rfUtilities)
require(parallel)
require(foreach)
require(doParallel)
require(ggplot2)
#require(functional)
require(plot3D)
require(dplyr)
require(tcltk)
require(caret)
require(randomForest)
require(ranger)
#require (OpenMP)
#require (randomForestSRC)
require (tools)
require(data.table)
require(spatstat)
require(spatialEco)
require(survey)
require(scales)
require(UBL)
#require (mcl3)
require(tidyr)
require (tidyverse)
require(rlang)
require (Rcpp)
require(sf)
require (mapview)

#install.packages ("haven")
```

# Import USA training point data 
CSV file with all variable climate data generated by ClimateNA for the 1960-91 Normal Period.
Several additional climate variables are calculated
```{r input data, echo=FALSE}

X1 <- fread("inputs/training_pts/US_TrainingPointsAll_21Nov2019_renamed_Normal_1961_1990MSY.csv",  stringsAsFactors = FALSE,data.table = FALSE)
## add some additional calculated variables
X1$PPT_MJ <- X1$PPT05 + X1$PPT06 # MaY/June precip
X1$PPT_JAS <- X1$PPT07 + X1$PPT08 + X1$PPT09 # July/Aug/Sept precip
X1$PPT.dormant <- X1$PPT_at + X1$PPT_wt # for calculating spring deficit
X1$CMD.def <- 500 - (X1$PPT.dormant)# start of growing season deficit original value was 400 but 500 seems better
X1$CMD.def [X1$CMD.def < 0] <- 0 #negative values set to zero = no deficit
X1$CMDMax <- X1$CMD07
X1$CMD.grow <- X1$CMD05 + X1$CMD06 +X1$CMD07 +X1$CMD08 +X1$CMD09
X1$CMD.total <- X1$CMD.def + X1$CMD

X1$tmaxSum20 <-  X1 %>%
      select( starts_with("Tmax")) %>% -20 %>%
              mutate_all(., list(~ifelse(. < 0, 0 , .))) %>%
                    rowSums(na.rm = TRUE)

X1$tmaxSum25 <-  X1 %>%
      select( starts_with("Tmax")) %>% -25 %>%
              mutate_all(., list(~ifelse(. < 0, 0 , .))) %>%
                    rowSums(na.rm = TRUE)

X1$tmaxSum30 <-  X1 %>%
      select( starts_with("Tmax")) %>% -30 %>%
              mutate_all(., list(~ifelse(. < 0, 0 , .))) %>%
                    rowSums(na.rm = TRUE)

X1$tmaxSum35 <-  X1 %>% 
      select( starts_with("Tmax")) %>% -35 %>%
              mutate_all(., list(~ifelse(. < 0, 0 , .))) %>% 
                    rowSums(na.rm = TRUE)

X1save = X1
X1 = X1save


```

``` {r test the variable importance in caret}
library(caret)
### do tests by Zone for relative simplicity rather than by subzone
# X1_test <- X1
# X1_test <- X1_test[,-c(1,3:5)]

left = function(text, num_char) {
  substr(text, 1, num_char)
}
X1$Zone <- left(X1$BGC, 4)
X1$Zone <- gsub("[[:lower:]]|[[:space:]]|[[:digit:]]", "", X1$Zone) ###for zone model
X1 <- X1[X1$Zone != "",]
X1$Zone <- as.factor(X1$Zone)
X1 <- droplevels(X1)
# X1_test$BGC <- gsub("  ","",X1_test$BGC) ## for subzone
# X1_test$BGC <- gsub(" ","",X1_test$BGC) ## for subzone
#X1_test$BGC <-  as.character(X1_test$BGC)
#X1_test <- X1_test %>%  filter(BGC == "BGdw_WA" | BGC == "BGmw_WA")
# X1_test$BGC <- as.factor(X1_test$BGC)
# X1_test <- droplevels(X1_test)
# X1_test$BGC <- sort (factor(as.character(X1_test$BGC)))
### caret preProcess function to reduce variables and convert to PCA
X1_test <- X1[,-c(1:5)]
X1_test_no_nzv_pca <- preProcess(select(X1_test, - Zone), 
                        method = c("nzv","corr"),
                        cutoff = .90)
X1_test_no_nzv_pca$method$remove
X1_test1 <- select(X1_test, -c(X1_test_no_nzv_pca$method$remove))

#### BioAll model - All Biologically Interpretable Variables Only:
model = "BioAll"

TEMP.list=c("Tmax_sp","Tmax_sm","Tmin_wt","Tmin_sp","Tmin_sm","Tmin_at",
            "DD5_sp","DD5_sm","DD5_at","DD5", "Tmax07","Tmax08",
            "MAT","MWMT","MCMT", "EMT","EXT", "DD5_06", "DD5_05", "TD")
#            "tmaxSum20", "tmaxSum25",,"tmaxSum30" ,"tmaxSum35" )
PPT.list=c("PPT06", "PPT_sp", "PPT_sm","MSP", "MAP","PAS", "PPT_MJ", "PPT_JAS")
OTHER.list=c("CMD_sp","CMD_sm","AHM","SHM","NFFD","bFFP","FFP","NFFD_sp", "CMD", "CMD.grow", "CMD07", "CMD.def", "CMD.total", "Eref")
ClimateVar=c(TEMP.list,PPT.list,OTHER.list)
List=c("Zone")
X1_Bio <- X1[,names(X1) %in% c(List,ClimateVar)]
X1_Bio_no_nzv_pca <- preProcess(select(X1_Bio, - Zone), 
                        method = c("nzv","corr"),
                        cutoff = .90)
X1_Bio_no_nzv_pca$method$remove
X1_Bio1 <- select(X1_Bio, -c(X1_Bio_no_nzv_pca$method$remove))
X1_Bio1 <-  drop_na (X1_Bio1)
#X1 <- X1[! X1$BGC == NA,]
X1_Bio1$Zone <- as.factor(X1_Bio1$Zone)
X1_Bio1 <- droplevels(X1_Bio1)

# Remove outliers for test
removeOutlier <- function(dat, alpha){
  out <- foreach(curr = unique(as.character(dat$BGC)), .combine = rbind) %do% {
    temp <- dat[dat$BGC == curr,]
    md <- tryCatch(mahalanobis(temp[,-1],center = colMeans(temp[,-1]), cov = cov(temp[,-1])),
                   error = function(e) e
    )
    if(!inherits(md,"error")){
      ctf <- qchisq(1-alpha, df = ncol(temp)-1)
      outl <- which(md > ctf)
      cat("Removing", length(outl), "outliers from",curr, "; ")
      if(length(outl) > 0){
        temp <- temp[-outl,]
      }
    }
    temp
    
  }
  return(out)
}
#X1.1 <- X1 %>% select(-Zone)
X1.sub <- removeOutlier(X1, alpha = 0.001)
X1 <- X1.sub
#X1.smote <- X1
#require(doParallel)
set.seed(123321)
coreNo <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(coreNo, cores = detectCores() - 1)
Cores <- as.numeric(detectCores()-1)

model_caret_raw <- train(Zone  ~ ., data = X1_test1,
                     method = "ranger",
                     trControl = trainControl(method="cv", number = 5, verboseIter = T, classProbs = T),
                     num.trees = 101,
                     importance = "impurity")

model_caret_scaled <- train(Zone  ~ ., data = X1_test1,
                     method = "ranger",
                     trControl = trainControl(method="cv", number = 5, verboseIter = T, classProbs = T),
                     num.trees = 101,
                     preProcess = c("center", "scale", "YeoJohnson"),#,tuneGrid = tgrid, 
                     importance = "impurity")
model_caret_PCA <- train(Zone  ~ ., data = X1_test1,
                     method = "ranger",
                     trControl = trainControl(method="cv", number = 5, verboseIter = T, classProbs = T),
                     num.trees = 101,
                     preProcess = c("pca"),#,tuneGrid = tgrid, 
                     importance = "impurity")

model_caret_bio_scaled <- train(Zone  ~ ., data = X1_Bio1,
                     method = "ranger",
                     trControl = trainControl(method="cv", number = 5, verboseIter = T, classProbs = T),
                     num.trees = 101,
                     preProcess = c("center", "scale", "YeoJohnson"),#,tuneGrid = tgrid, 
                     importance = "impurity")

model_SVM_scaled <- train(Zone  ~ ., data = X1_Bio1,
                     method = "svmLinear",
                     trControl = trainControl(method="cv", number = 5, verboseIter = T, classProbs = T),
                     num.trees = 101,
                     preProcess = c("center", "scale", "YeoJohnson"),#,tuneGrid = tgrid, 
                     importance = "impurity")
model_XGB_scaled <- train(Zone  ~ ., data = X1_Bio1,
                     method = "xgbTree",
                     trControl = trainControl(method="cv", number = 5, verboseIter = T, classProbs = T),
                     num.trees = 101,
                     preProcess = c("center", "scale", "YeoJohnson"),#,tuneGrid = tgrid, 
                     importance = "impurity")
model_NNET_scaled <- train(Zone  ~ ., data = X1_Bio1,
                     method = "nnet",
                     trControl = trainControl(method="cv", number = 5, verboseIter = T, classProbs = T),
                     num.trees = 101,
                     preProcess = c("center", "scale", "YeoJohnson"),#,tuneGrid = tgrid, 
                     importance = "impurity")


 stopCluster(coreNo)
 gc()
results <- resamples(list(Raw=model_caret_raw, Scaled=model_caret_scaled, Bio = model_caret_bio_scaled, XGB = model_XGB_scaled))
#,PCA = model_caret_PCA, SVM = model_SVM_scaled,  model_NNET_scaled))
summary(results)
model_compare <- summary(results)
bwplot(results)
dotplot(results)
#   print(model_caret)
 plot (varImp(model_caret_bio_scaled), top=20)
# model_caret$results


```
#Select model and parameters from previous chunk
Initialy reduced to only biologically meaningful variables (42)
Then test and remove any variables with near zero variance (this is important for CV processes but may miss "special" variables that define a minority class)
Then remove highly correlated variables using the caret function findCorrelation

```{r reduce variable set, echo=FALSE}
#### BioAll model - All Biologically Interpretable Variables Only:
model = "BioAll"

TEMP.list=c("Tmax_sp","Tmax_sm","Tmin_wt","Tmin_sp","Tmin_sm","Tmin_at",
            "DD5_sp","DD5_sm","DD5_at","DD5", "Tmax07","Tmax08",
            "MAT","MWMT","MCMT", "EMT","EXT", "DD5_06", "DD5_05", "TD")
#            "tmaxSum20", "tmaxSum25",,"tmaxSum30" ,"tmaxSum35" )
PPT.list=c("PPT06", "PPT_sp", "PPT_sm","MSP", "MAP","PAS", "PPT_MJ", "PPT_JAS")
OTHER.list=c("CMD_sp","CMD_sm","AHM","SHM","NFFD","bFFP","FFP","NFFD_sp", "CMD", "CMD.grow", "CMD07", "CMD.def", "CMD.total", "Eref")
ClimateVar=c(TEMP.list,PPT.list,OTHER.list)
List=c("Zone")
X1_Bio <- X1[,names(X1) %in% c(List,ClimateVar)]
X1_Bio_no_nzv_pca <- preProcess(select(X1_Bio, - Zone), 
                        method = c("nzv","corr"),
                        cutoff = .90)
X1_Bio_no_nzv_pca$method$remove
X1_Bio1 <- select(X1_Bio, -c(X1_Bio_no_nzv_pca$method$remove))
X1_Bio1 <-  drop_na (X1_Bio1)
#X1 <- X1[! X1$BGC == NA,]
X1_Bio1$Zone <- as.factor(X1_Bio1$Zone)
X1_Bio1 <- droplevels(X1_Bio1)
X1_Bio1 <-X1_Bio1  %>% select(Zone, everything()) 
# Remove outlier training points.
# Looks for training points which fall climatically well outside of the normal range by BGC.
# Mostly aimed at catching the 10% of FIA plots which have been given an intentionally offset georeferenced location
# May also indicate overly broad BGCs (eg. IMAus)
removeOutlier <- function(dat, alpha){
  out <- foreach(curr = unique(as.character(dat$Zone)), .combine = rbind) %do% {
    temp <- dat[dat$Zone == curr,]
    md <- tryCatch(mahalanobis(temp[,-1],center = colMeans(temp[,-1]), cov = cov(temp[,-1])),
                   error = function(e) e
    )
    if(!inherits(md,"error")){
      ctf <- qchisq(1-alpha, df = ncol(temp)-1)
      outl <- which(md > ctf)
      cat("Removing", length(outl), "outliers from",curr, "; ")
      if(length(outl) > 0){
        temp <- temp[-outl,]
      }
    }
    temp
    
  }
  return(out)
}
#X1.1 <- X1 %>% select(-Zone)
X1_Bio2 <- removeOutlier(X1_Bio1, alpha = .001)
# Test models between all training points and where outliers removed
set.seed(123321)
coreNo <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(coreNo, cores = detectCores() - 1)
Cores <- as.numeric(detectCores()-1)

BGCmodel1 <- train(Zone  ~ ., data = X1_Bio1,
                     method = "ranger",
                     trControl = trainControl(method="cv", number = 10,  verboseIter = T, classProbs = T),
                     #num.trees = 101, 
                     preProcess = c("center", "scale", "YeoJohnson"),#,tuneGrid = tgrid, 
                     importance = "impurity")

BGCmodel2 <- train(Zone  ~ ., data = X1_Bio2,
                     method = "ranger",
                     trControl = trainControl(method="cv", number = 10, verboseIter = T, classProbs = T),
                     #num.trees = 101, 
                     preProcess = c("center", "scale", "YeoJohnson"),#,tuneGrid = tgrid, 
                     importance = "impurity")

# BGCmodel3 <- train(Zone  ~ ., data = X1_Bio2,
#                      method = "ranger",
#                      trControl = trainControl(method="cv", number = 10, verboseIter = T, classProbs = T),
#                     # num.trees = 101, 
#                      preProcess = c("center", "scale", "YeoJohnson"),#,tuneGrid = tgrid, 
#                      importance = "impurity")



 stopCluster(coreNo)
 gc()
results <-resamples( list(All=BGCmodel1, Outliers=BGCmodel2))#
#,PCA = model_caret_PCA, SVM = model_SVM_scaled,  model_NNET_scaled))
summary(results)
model_compare <- summary(results)
bwplot(results)
dotplot(results)
#   print(model_caret)
 plot (varImp(model_caret_bio_scaled), top=20)

```

# Test model build using SMOTE on training point set

``` {r build model of Zones}
# 
# tpNum <- 750 ##number of desired training points per unit
# mList <- list()
# for(Zone in unique(X1$Zone)){##Calculate multiplier
#   num <- length(X1$Zone[X1$Zone == Zone])
#   mNum <- tpNum/num
#   mList[[Zone]] <- mNum
# }
# 
# ###scaled by log10
# mList <- list()
# for(BGC in unique(X1$Zone)){##Calculate multiplier
#   num <- length(X1$Zone[X1$Zone == Zone])
#   tpNum <- log10(num)*300
#   mNum <- tpNum/num
#   mList[[Zone]] <- mNum
# }
# 
# ###SMOTE!
# X1.smote <- SmoteClassif(Zone ~ ., X1[-1], C.perc = mList, k= 5 , repl = TRUE, dist = "Euclidean")
X1.smote <- X1[-1]
# table(X1.smote$BGC)

# BGCmodel <- randomForest(Zone ~ ., data=X1.smote, nodesize = 5, do.trace = 10,
#                          ntree=101, na.action=na.fail, importance=TRUE, proximity=FALSE)
# 
# set.seed(123321)
# coreNo <- makePSOCKcluster(detectCores() - 1)
# registerDoParallel(coreNo, cores = detectCores() - 1)
# Cores <- as.numeric(detectCores()-1)
# 
# BGCmodel <- train(Zone  ~ ., data = X1.smote,
#                      method = "ranger",
#                      trControl = trainControl(method="cv", number = 10, verboseIter = T, classProbs = T),
#                 
#                      importance = "permutation")
# varImp(BGCmodel)
# BGCmodel$results
# 
#  stopCluster(coreNo)
#  gc()

```

####Predict zone membership of hex grid
### Use the best model from those above (Scaled, Outliers Removed)
``` {r Build }


grid.dat <- fread("./inputs/grids/USA_8000m_Grid_Climate_new_Normal_1961_1990MSY.csv", stringsAsFactors = FALSE,  data.table = FALSE)
Z1 <- grid.dat
model = "BioAll"

Z1$PPT_MJ <- Z1$PPT05 + Z1$PPT06 # MaY/June precip
Z1$PPT_JAS <- Z1$PPT07 + Z1$PPT08 + Z1$PPT09 # July/Aug/Sept precip
Z1$PPT.dormant <- Z1$PPT_at + Z1$PPT_wt # for calculating spring deficit
Z1$CMD.def <- 500 - (Z1$PPT.dormant)# start of growing season deficit original value was 400 but 500 seems better
Z1$CMD.def [Z1$CMD.def < 0] <- 0 #negative values set to zero = no deficit
Z1$CMDMax <- Z1$CMD07
Z1$CMD.grow <- Z1$CMD05 + Z1$CMD06 +Z1$CMD07 +Z1$CMD08 +Z1$CMD09
Z1$CMD.total <- Z1$CMD.def + Z1$CMD
TEMP.list=c("Tmax_sp","Tmax_sm","Tmin_wt","Tmin_sp","Tmin_sm","Tmin_at",
            "DD5_sp","DD5_sm","DD5_at","DD5", "Tmax07","Tmax08",
            "MAT","MWMT","MCMT", "EMT","EXT", "DD5_06", "DD5_05", "TD")
#            "tmaxSum20", "tmaxSum25",,"tmaxSum30" ,"tmaxSum35" )

colnames(Z1)[1:2] <- c("ID1","ID2")
# ####New 16 Variable Set
# VarList = c("TD",	"Eref",	"AHM",	"Tmax07",	"CMD.total",	"Tmax_sp","Tmin_sm","CMD_sp","PAS",	
#             "PPT_sp","PPT06","CMD.def","DD5_sp","Tmin_at","bFFP","MCMT")
List = c("ID1",  "Latitude", "Longitude", "Elevation")
grid.dat = Z1[,names(Z1) %in% c(List, X1.var)]
grid.dat2 <- grid.dat
###Predict
grid.dat$Zone <- predict(BGCmodel, newdata = grid.dat[,-c(1:4)])
grid.zone <- dplyr::select(grid.dat, ID1, Zone, Latitude, Longitude,Elevation )#"Longitude","Latitude", "Elevation", "Zone")]

PPT.list=c("PPT06", "PPT_sp", "PPT_sm","MSP", "MAP","PAS", "PPT_MJ", "PPT_JAS")
OTHER.list=c("CMD_sp","CMD_sm","AHM","SHM","NFFD","bFFP","FFP","NFFD_sp", "CMD", "CMD.grow", "CMD07", "CMD.def", "CMD.total", "Eref")
ClimateVar=c(TEMP.list,PPT.list,OTHER.list)
List=c("Zone")
X1_Bio <- X1[,names(X1) %in% c(List,ClimateVar)]
X1_Bio_no_nzv_pca <- preProcess(select(X1_Bio, - Zone), 
                        method = c("nzv","corr"),
                        cutoff = .90)
X1_Bio_no_nzv_pca$method$remove
X1_Bio1 <- select(X1_Bio, -c(X1_Bio_no_nzv_pca$method$remove))
X1_Bio1 <-  drop_na (X1_Bio1)
#X1 <- X1[! X1$BGC == NA,]
X1_Bio1$Zone <- as.factor(X1_Bio1$Zone)
X1_Bio1 <- droplevels(X1_Bio1)
X1_Bio1 <-X1_Bio1  %>% select(Zone, everything()) 
# Remove outlier training points.
# Looks for training points which fall climatically well outside of the normal range by BGC.
# Mostly aimed at catching the 10% of FIA plots which have been given an intentionally offset georeferenced location
# May also indicate overly broad BGCs (eg. IMAus)
removeOutlier <- function(dat, alpha){
  out <- foreach(curr = unique(as.character(dat$Zone)), .combine = rbind) %do% {
    temp <- dat[dat$Zone == curr,]
    md <- tryCatch(mahalanobis(temp[,-1],center = colMeans(temp[,-1]), cov = cov(temp[,-1])),
                   error = function(e) e
    )
    if(!inherits(md,"error")){
      ctf <- qchisq(1-alpha, df = ncol(temp)-1)
      outl <- which(md > ctf)
      cat("Removing", length(outl), "outliers from",curr, "; ")
      if(length(outl) > 0){
        temp <- temp[-outl,]
      }
    }
    temp
    
  }
  return(out)
}
#X1.1 <- X1 %>% select(-Zone)
X1_Bio2 <- removeOutlier(X1_Bio1, alpha = .001)## add some additional calculated variables



hexpoly <- st_read(dsn = "./inputs/hex_shapes", layer = "USA_HexPoly_8000m_new")
##testpnts <- pointDat[pointDat$Latitude > 43.407 & pointDat$Latitude < 46.5 & pointDat$Longitude < -111.79 & pointDat$Longitude > -115.412,]##subset to small area for example
hexZone <- left_join(hexpoly, grid.zone, by = "ID1")
temp <- select(hexZone, Zone, geometry)
# testpnts <- grid.zone
# testpnts <- testpnts[,c("Longitude","Latitude","Zone")]
# testpnts <- st_as_sf(testpnts, coords = c("Longitude","Latitude"), crs = 4326, agr = "constant")##convert to spatial object
# testpnts <- st_transform(testpnts, crs = st_crs(hexpoly))
# temp <- st_join(hexpoly,testpnts)
# temp <- temp[!is.na(temp$Zone),]
#temp <- temp[,-1]

temp2 <- temp
st_precision(temp2) <- 0.5###union polygons within zone
t1 <- temp2 %>%
  group_by(Zone) %>%
  summarise(geometry = sf::st_union(geometry)) %>%
  ungroup()
mapView(t1)

st_write(t1, dsn = "outputs", layer = "rawZoneMap_800m", driver = "ESRI Shapefile", update = TRUE)

```
# clean up isolated individual pixels
``` {r cleanup up crumbs}

###now cleanup and remove crumbs
t2 <- st_cast(t1, "MULTIPOLYGON") %>% st_cast("POLYGON")
t2 <- t2 %>%
  mutate(Area = st_area(.)) %>%
  mutate(ID = seq_along(Zone))

library(units)
size <- 560000
size <- set_units(size, "m^2")
tSmall <- t2[t2$Area < size,]
t2$Zone <- as.character(t2$Zone)

require(doParallel)
coreNum <- as.numeric(detectCores()-1)
coreNo <- makeCluster(coreNum)
registerDoParallel(coreNo, cores = coreNum)

###loop through each polygon < size, determine intersects, and assign to zone with most edge touching
###all the built in functions I found only dealt with holes in the middle of polygons
new <- foreach(i = 1:length(tSmall$ID), .combine = rbind, .packages = c("foreach","sf")) %dopar% {
  ID <- tSmall$ID[i]
  nbrs <- st_intersects(tSmall[i,],t2)[[1]]
  nbrs <- nbrs[!nbrs %in% ID]
  if(length(nbrs) == 0){return(NULL)}
  lines <- st_intersection(t2[ID,],t2[nbrs,])
  lines <- st_cast(lines)
  l.len <- st_length(lines)
  names(l.len) <- lines$Zone.1
  zn <- names(l.len)[l.len == max(l.len)][1]
  newDat <- t2[ID,]
  newDat$Zone <- zn
  newDat
}

stopCluster(coreNo)
gc()
temp <- t2[!t2$ID %in% new$ID,]
t2 <- rbind(temp, new) %>%
  mutate(Zone = as.factor(Zone))

###now have to combine crumbs with existing large polygons
temp2 <- t2
st_precision(temp2) <- 0.5
t1 <- temp2 %>%
  group_by(Zone) %>%
  summarise(geometry = sf::st_union(geometry)) %>%
  ungroup()

mapview(t1, zcol = "Zone")
```


# Build Subzone models and predict within each Zone

``` {r Build Subzone model and predict}
####now subzones
#t1 <- st_read(dsn = "outputs", layer = "rawZoneMap")
pointDat <- grid.dat
######################################################

X2 <-  X1
#X2 <- X1 %>%  filter(Zone == "ICH" | Zone == "BG")
#X2 <- X1 %>%  filter(Zone == "BG" | Zone == "FG" |  Zone == "GBD" |  Zone == "MGP" |  Zone == "SGP" |  Zone == "MDCH")
X2 <- X1 %>%  filter(Zone == "IMA" | Zone == "CMA" )
Zones <- as.character(unique(X2$Zone))
#Zones <- as.character(unique(pointDat$Zone))
#Zones <- sort (Zones)

#X2 <- droplevels(X2)

SZPred <- foreach(Z = Zones, .combine = rbind) %do% {
  trainSub <- X2[X2$Zone == Z,]###subset training points to only include selected zone
   #trainSub$BGC <- droplevels(trainSub$Zone)
   if(length(unique(trainSub$BGC)) >= 2){ ###somtimes there aren't any subzones
    trainSub$BGC <- as.factor(trainSub$BGC)
    trainSub$BGC <- droplevels(trainSub$BGC)
    trainSub <- removeOutlier(trainSub, alpha = 0.001)
    mList <- list()
    # for(BGC in unique(trainSub$BGC)){##Calculate multiplier
    #   num <- length(trainSub$BGC[trainSub$BGC == BGC])
    #   tpNum <- log10(num)*150
    #   mNum <- tpNum/num
    #   mList[[BGC]] <- mNum
    # }
    trainSub <- SmoteClassif(BGC ~ ., trainSub [-19], C.perc = mList, k= 5 , repl = TRUE, dist = "Euclidean")
    
 # drop = "Zone"
      SZmodel <- randomForest(BGC ~ ., data=trainSub [-19], nodesize = 5, do.trace = 10, 
                            ntree=101, na.action=na.fail, importance=TRUE, proximity=FALSE)
    
    pointSub <- pointDat[pointDat$Zone == Z,] ###subset grid based on previously predicted zones
    pointSub$BGC<- predict(SZmodel, newdata = pointSub[,-c(1:4,22),]) ### predict subzones
    
    out <- pointSub[,c("ID1", "Latitude","Longitude","Zone", "BGC")]
    out
  }else{ ##if only one subzone, plot 
    pointSub <- pointDat[pointDat$Zone == Z,]
    #pointSub$Zone <- droplevels(pointSub$Zone)
    pointSub$BGC <- pointSub$Zone
    out <- pointSub[,c("ID1", "Latitude","Longitude","Zone", "BGC")]
    out
  }
  
}

grid.sbz <- dplyr::select(SZPred, ID1, BGC)#"Longitude","Latitude", "Elevation", "Zone")]
table(grid.sbz$BGC)
grid.sbz <- droplevels(grid.sbz)
hexZone2 <- left_join(hexZone, grid.sbz, by = "ID1")
#temp <- select(hexZone, Zone, geometry)
temp <- hexZone2
temp$BGC <-  fct_explicit_na(temp$BGC , na_level = "(None)")
table(temp$BGC)

# grid.sbz <- SZPred[,c("Longitude","Latitude","BGC")]
# grid.sbz  <- st_as_sf(testpnts, coords = c("Longitude","Latitude"), crs = 4326, agr = "constant")##convert to spatial object
# grid.sbz  <- st_transform(grid.sbz , crs = st_crs(hxPnts))
# temp <- st_join(hxPnts,grid.sbz )
# temp <- temp[!is.na(temp$BGC),]
# temp <- temp[,-1]

temp2 <- temp
st_precision(temp2) <- 0.5###union polygons within zone
t3 <- temp2 %>%
  group_by(BGC) %>%
  summarise(geometry = sf::st_union(geometry)) %>%
  ungroup()

mapview(t3)

st_write(t3, dsn = "outputs", layer = "Alpine_SubzoneRaw", driver = "ESRI Shapefile", update = TRUE)


# hexZone <- left_join(hexpoly, grid.sbz , by = "ID1")
# temp <- select(hexZone, BGC, geometry)
# # testpnts <- grid.zone
# # testpnts <- testpnts[,c("Longitude","Latitude","Zone")]
# # testpnts <- st_as_sf(testpnts, coords = c("Longitude","Latitude"), crs = 4326, agr = "constant")##convert to spatial object
# # testpnts <- st_transform(testpnts, crs = st_crs(hexpoly))
# # temp <- st_join(hexpoly,testpnts)
# # temp <- temp[!is.na(temp$Zone),]
# #temp <- temp[,-1]
# 
# temp2 <- temp
# st_precision(temp2) <- 0.5###union polygons within zone
# t3 <- temp2 %>%
#   group_by(Zone) %>%
#   summarise(geometry = sf::st_union(geometry)) %>%
#   ungroup()
# mapView(t3)
# 
# st_write(t3, dsn = "outputs", layer = "rawZoneMap", driver = "ESRI Shapefile", update = TRUE)

```

##Build machine learning model

```{r plots of variables}

require(doParallel)
set.seed(123321)
coreNo <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(coreNo, cores = detectCores() - 1)
Cores <- as.numeric(detectCores()-1)
 control <- trainControl(method = 'cv', number = 2,  allowParallel = TRUE)
   BGCmodel <- train(BGC ~ ., data = X1.sub,  method ='rf', trControl = control, ntree = 21)
                     #, do.trace = 10, ntree = 11)
                         # ntree=71, mtry=3, nodesize = 5, metric = metric, na.action=na.omit, importance=FALSE, proximity=FALSE)#strata=BGC, sampsize= c(500),
 stopCluster(coreNo)
 gc()
  print(BGCmodel)
  
  rf.imp <- varImp(BGCmodel, scale = FALSE)
rf.imp
  ##may need to change BGC for Zone where running for entire model of for subzone by zone foreach loop
# ptm <- proc.time()
# BGCmodel <- ranger(BGC ~ ., data=X1, verbose = TRUE, 
#                         num.trees=71, importance="impurity")
# proc.time() - ptm
fit.rf <- randomForest(BGC ~ ., data=X1.sub2,  do.trace = 10, mtry= 3,
                       ntree=71, na.action=na.omit, importance=TRUE, proximity=FALSE)

 varImpPlot(fit.rf, sort=TRUE, n=40)  

    ###caret package version
Seed <- 1234
metric <- "Kappa"
 control <- trainControl(method = 'repeatedcv', number = 2, repeats = 2, allowParallel = TRUE)
  BGCmodel <- train(BGC ~ ., data=X1.sub,  method='rf', trControl = control, do.trace = 10, 
                          ntree=71, mtry=3, nodesize = 5, metric = metric, na.action=na.omit, importance=FALSE, proximity=FALSE)#strata=BGC, sampsize= c(500),

 #metric <- "Accuracy"

# Save Confusion matrix
print(BGCmodel$confusion, digits=2)
#Novel <-outlier(BGCmodel, X1)
#fname <- "BGCv11_AB_USA_500each_27VAR_SubZone"
fname <- "BC_var16"
model = "Oct22"
write.csv(BGCmodel$proximity, file= paste(fname,"_Proximity",model,".csv",sep=""))
write.csv(BGCmodel$importance, file= paste(fname,"_Importance",model,".csv",sep=""))
write.csv(BGCmodel$err.rate, file= paste(fname,"_Error",model,".csv",sep=""))
write.csv(BGCmodel$confusion, file= paste(fname,"_ConfusionMatrix",model,".csv",sep=""))
write.csv(BGCmodel$confusion[, 'class.error'], file= paste(fname,"_Confusion",model,".csv",sep=""))
VIP <- varImpPlot(BGCmodel, sort=TRUE) 
write.csv(VIP, file= paste(fname,"_OBO",model,".csv",sep=""))
dev.copy(pdf,(paste(model,'VarImpPlot.pdf')))
dev.off()


###BGCmodel from ranger implementation of randomForest *****not much faster
ptm <- proc.time()
BGCmodel_ran <- ranger(BGC ~ ., data=X1,  mtry=NULL, write.forest = TRUE, num.trees=11, importance='impurity', classification = TRUE)
proc.time() - ptm

#### Save random forest model
file=paste(fname,"_RFmodelv4",".Rdata",sep="")
save(BGCmodel,file=file)

# Save Confusion matrix
print(BGCmodel$confusion, digits=2)
write.csv(BGCmodel$err.rate, file= paste(fname,"_ErrorBio2v1000_53",".csv",sep=""))
write.csv(BGCmodel$confusion, file= paste(fname,"_ConfusionMatrixBio2v1000_53",".csv",sep=""))
write.csv(BGCmodel$confusion[, 'class.error'], file= paste(fname,"_ConfusionBio2v1000_53",".csv",sep=""))
varImpPlot(BGCmodel, sort=TRUE, n.var=nrow(BGCmodel$importance))

###Runs iterations to determine how many variables are required for accurancy
ptm <- proc.time()
BGCmodelVar <- rfcv(trainx = X1[,-1], trainy = X1[,1], scale = "log", step=0.5)
proc.time() - ptm
    ###graph of decrease in error with number of variables
with(BGCmodelVar, plot(n.var, error.cv,log="x", type="o", lwd=2))
print(BGCmodelVar$predicted)
save(BGCmodelVar$predicted,file=paste(fname,"_VarImp",".JPEG",sep=""))
      
 ##### run the next line of code iteratively to find best mtry number
BGCmodel2 <- tuneRF(X1[,-1], X1[,1], mtryStart=5, ntreeTry=151, stepFactor=1.5, improve=0.05,trace=TRUE, plot=TRUE, doBest=FALSE)



#stopCluster(coreNo)
##############Parallel version
# ptm <- proc.time()
# 
# Zonemodel <- foreach(ntree = rep(15,7), .combine = randomForest::combine, .multicombine = TRUE, .packages = "randomForest") %dopar% {
#   randomForest(X1[-1], X1[1],
#                ntree=ntree, na.action=na.fail, importance=TRUE, proximity=FALSE)
# }
# proc.time() - ptm
   #### Save random forest model
file=paste("USA_Subzones_RFModel_16Var_20Nov2019",".Rdata",sep="")
save(BGCmodel,file=file)
#load("USA_Subzones_RFModel_16Var_20Nov2019.Rdata")
#load("USA_Subzones_RFModel_27Var.Rdata")
#load(file.choose())

```


#####Predict BGC units##########################
```{r predict BGC for hexgrid}
#load("BGCv11_AB_USA_LHC_10VAR_SubZone_RFmodel.Rdata") ##load random forest model
### Read in US Hex Grid file with normal data. SOMETIMES THE FIRST 2 COLUMN NAMES NEED TO BE CHANGED
Columns = c("rn", "Region", "Latitude", "Longitude", "Elevation", "AHM", "bFFP",
            "CMD07","DD5_sp","EMT","Eref_sm","EXT","FFP","MCMT","MSP",
            "PPT07","PPT08", "PPT05","PPT06","PPT09", "SHM","TD","Tmax_sp","Tmin_at",
            "Tmin_sm","Tmin_wt", "PPT_at","PPT_wt", "PAS","eFFP",
            "Eref09","MAT","Tmin_sp","CMD")#"GCM", 

pointDat <- fread(file.choose(), stringsAsFactors = FALSE,  data.table = FALSE)#, select = Columns
#pointDat <- pointDat[pointDat$DD_0_01 != -9999,]
#pointDat <- pointDat[pointDat$Latitude < 49,]
X1 <-pointDat
### add additional variables to match rF model
X1$PPT_MJ <- X1$PPT05 + X1$PPT06 # MaY/June precip
X1$PPT_JAS <- X1$PPT07 + X1$PPT08 + X1$PPT09 # July/Aug/Sept precip
X1$PPT.dormant <- X1$PPT_at + X1$PPT_wt # for calculating spring deficit
X1$CMD.def <- 500 - (X1$PPT.dormant)# start of growing season deficit original value was 400 but 500 seems better
X1$CMD.def [X1$CMD.def < 0] <- 0 #negative values set to zero = no deficit
X1$CMDMax <- X1$CMD07
X1$CMD.total <- X1$CMD.def + X1$CMD
X1save = X1
X1 = X1save
colnames(X1)[1]=c("ID1")
colnames(X1)[2]=c("ID2")
#####New 16 Variable list to Use.
VarList = c("TD",	"Eref",	"AHM",	"Tmax07",	"CMD.total",	"Tmax_sp","Tmin_sm","CMD_sp","PAS",	
            "PPT_sp","PPT06","CMD.def","DD5_sp","Tmin_at","bFFP","MCMT")#16Var
#VarList = c("AHM", "bFFP","CMD.total","DD5_sp","EMT","Eref_sm","EXT","FFP","MCMT","MSP",
#            "PPT_JAS","PPT_MJ","PPT06","SHM","TD","Tmax_sp","Tmin_at","Tmin_sm","Tmin_wt",
#            "PAS","CMD.def","CMDMax","eFFP","Eref09","MAT","PPT07","Tmin_sp")
List = c("ID1","ID2", "Latitude", "Longitude", "Elevation")
X1.sub = X1
X1.sub=X1[,names(X1) %in% c(List,VarList)]
X1.sub <- X1.sub[!X1.sub[,6] == -9999.0,]
X1 <- X1.sub
pointDat <-X1.sub
#pointDat <- Y1
###Predict BGC units
ranger.pred <- predict(BGCmodel, data = pointDat[,-c(1:5)])
pointDat$BGC.pred <- ranger.pred$predictions
 pointDat.saved <- pointDat
 pointDat <-pointDat.saved
 #points <- pointDat[,c("Latitude","Longitude","Zone")]
 pointDat$BGC.pred <- as.character(pointDat$BGC.pred)
 pointDat$Subzone <- gsub("[[:space:]]","",pointDat$BGC.pred)##for subzones
  pointDat$Zone <- word(pointDat$BGC.pred,1,sep = "\\_")
 pointDat$Zone <- gsub("[[:lower:]]|[[:space:]]|[[:digit:]]","",pointDat$Zone)##for just zone
points <- pointDat[,c("ID1", "Zone", "Subzone", "Latitude","Longitude","Elevation")]
write.csv (points, "TestPredictedPoints.csv")
List2 = c("ID1", "Zone", "Subzone", "Latitude","Longitude","Elevation")
#List2 = c("ID1", "Subzone")
pointLHC <- pointDat[,names(pointDat) %in% c(List2,VarList)]###includes all the climate variables for LHC procedure below
colnames (pointLHC) [18] <- "ID2"
col_order <-c("ID1", "Zone", "Subzone", "Latitude","Longitude","Elevation","CMD.total", "bFFP","Eref_sm","MCMT","PPT_JAS",
              "SHM","TD","PAS","DD5_sp","PPT06")
pointLHC <- pointLHC[,col_order]
#colnames(points)[2]<- "ZoneSubzone"
#colnames(points)[1]<- "ID1"
```

###Write to CSV to import into QGIS and join to HexGrid

``` {r write predicted point data to CSV}
write.csv (points, "USA_FAIPlots_BGCPredicted.csv")
write.csv (pointLHC, "USA_1k_HexGridPts_BGC_predicted_16Var_LHC_May3_w_BGCdata.csv")
countZone <- points %>% count(Zone)
countSubzone <- points %>% count(Subzone)
countSubzone$logn <- log(countSubzone$n, 10)
countSubzone$rs <- as.integer (rescale(countSubzone$logn, to = c(300, 1000), from = range(countSubzone$logn, na.rm = TRUE, finite = TRUE)))
countSubzone$sample <- ifelse(countSubzone$rs > countSubzone$n, countSubzone$n, countSubzone$rs )
allUnits <- unique(points$Subzone)
write.csv (countSubzone, "USA_ptPerBECLHC2.csv")

###########From here add CSV to QGIS and Join by ID1 to HexPolygon Shape file
```  

##############select a subset of training points   
``` {r select a subsample of training points from each predicted BGC for use in WNA model}
library(clhs)
BGC = "ICHxwz"
allUnits <- unique(pointLHC$Subzone)
pointLHC$ID1 <- row.names(pointLHC)
LHCtraining <- foreach(BGC = allUnits, .combine = rbind, .packages = c("clhs")) %dopar% {
temp <- pointLHC[(pointLHC$Subzone %in% BGC),]
Num <- countSubzone$sample[(countSubzone$Subzone %in% BGC)]
  samples <- clhs(temp[,-c(1:6)], 
                size = Num,           # Test a range of sample sizes
                iter = 100,        # Arbitrarily large number of iterations of optimization procedure. Default=10,000 but a larger number may be used
                progress = TRUE, 
                simple = FALSE)

cLHS <- samples$sampled_data
cLHS$ID1 <- row.names (cLHS)
cLHS_Points <- merge (pointLHC[,c(1:6)], cLHS, by = "ID1")
cLHS_Points
}

write.csv(LHCtraining, "USA_Training_LHC.csv")
X1 <- LHCtraining #feed these back into building a new rF model above


