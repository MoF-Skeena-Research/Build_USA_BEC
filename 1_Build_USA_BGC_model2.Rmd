---
title: "Model USA Biogeoclimatic Units using Machine Learning"
author: "William H MacKenzie and Kiri Daust"
date: "17/10/2019"
output: 
   html_document:
  code_folding: hide
  #theme: flatly
---
# Script to create RF model from USA training points and predict + map US zones/ subzones (predicted within each zone)
Import training data
Reduce variable list with caret or PCA all variable
Build machine learning model
Test for missing variable space or map error for required additional training points
Loop to build models by zone and then by subzone within zone to predict map
Translates predictions to a grid point map
Add predictions to hex polygon layer.
Overlay additional plots over map to assign to USA_BGC units


# STEP 1: Prepare dataset for analysis----clear workspace
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
require(rattle)
require(rpart)
require(rpart.plot)
require(plyr)
require(reshape)
require(reshape2)
#require(VSURF)
#require(reports)
#require(rpart.utils)
#require(rfUtilities)
require(parallel)
require(foreach)
require(doParallel)
require(ggplot2)
#require(functional)
require(plot3D)
require(dplyr)
require(tcltk)
require(caret)
require(randomForest)
require(ranger)
#require (OpenMP)
#require (randomForestSRC)
require (tools)
require(data.table)
require(spatstat)
require(spatialEco)
require(survey)
require(scales)
require(UBL)
#require (mcl3)
require(tidyr)
require (tidyverse)
require(rlang)
require (Rcpp)
require(sf)
require (mapview)
require(forcats)
require(Boruta)
require(purrrlyr)
require(skimr)
require(gbm)
require(vcd)
require(alluvial)
require(clhs)
require(smotefamily)
require(smoothr)
require(tictoc)
require(tidymodels)
#install.packages ("clhs")
```

# Import USA training point data 
CSV file with all variable climate data generated by ClimateNA for the 1960-91 Normal Period.
Several additional climate variables are calculated
```{r input data, echo=FALSE}
###read in training point data - update BGC with revise classification if required
X1 <- fread("./inputs/training_pts/US_TrainingPointsAll_27Dec2020_Normal_1961_1990MSY.csv",  stringsAsFactors = FALSE,data.table = FALSE) %>% filter(!BGC == "NA")

## update BGC from new table with updated BGC if assigned
#X1 <- X1 %>% dplyr::select(-BGC)
#X1_new <- fread("./inputs/training_pts/US_TrainingPoints_27Dec2020.csv",  stringsAsFactors = FALSE,data.table = FALSE)
#X1_new <- X1_new %>% dplyr::select(ID1, BGC)
#X1 <- left_join(X1, X1_new, by = "ID1") %>% filter(!BGC == "NA")

##temporary updates
# X1_new2 <- fread("./inputs/training_pts/CWHvm_to_CWHms_2.csv",  stringsAsFactors = FALSE,data.table = FALSE)
# X1_new2$BGC <- recode(X1_new2$BGC, "CWHvm_WA" = "CWHms_WA") 
# X1_new2 <- X1_new2 %>% dplyr::select(ID1, BGC) 
# X1_new <- left_join(X1_new, X1_new2, by = "ID1")
# X1_new$BGC.y <- ifelse(is.na(X1_new$BGC.y), X1_new$BGC.x, X1_new$BGC.y)
# X1_new <- X1_new %>% select(ID1, BGC.y) %>% rename(BGC = BGC.y)
## update BGC from new table with updated BGC



##Add in border points from BC and Alberta from WNA 4k Grid data
Y1 <- fread("./inputs/training_pts/WNA_4k_HexPts_BGC_Normal_1961_1990MSY.csv",  stringsAsFactors = FALSE,data.table = FALSE)
Y2 <- Y1  %>% filter(Latitude < 49.5) #%>% filter(BGC %in% c("CDFmm", "ICHdm", "IDFxx1", "CWHms1", "MSdm2"))
#Y3 <- Y1 %>% filter(Latitude > 49) %>% filter(Latitude < 49.5)
X1 <- rbind(X1,Y2) %>% select(-ID1) %>%  rowid_to_column("ID1")
#X1 <- rbind(X2,Y3)

X1_loc <- X1 %>% dplyr::select(ID1, Latitude, Longitude, Elevation)
X1 <- X1 %>% dplyr::select (ID1, BGC, everything()) %>% dplyr::select(-Latitude, -Longitude, -Elevation)

# X2 <- fread("./cLHS/WNA_Training_22_VARS.csv", stringsAsFactors = FALSE, data.table = FALSE)
# X3 <- left_join(X2, X1) %>% dplyr::select(ID1, BGC, Latitude, Longitude, Elevation)
# write.csv(X3, "LHS_Training_Locations.csv")

addVars <- function(dat){
  dat$PPT_MJ <- dat$PPT05 + dat$PPT06  # MaY/June precip
  dat$PPT_JAS <- dat$PPT07 + dat$PPT08 + dat$PPT09  # July/Aug/Sept precip
  dat$PPT.dormant <- dat$PPT_at + dat$PPT_wt  # for calculating spring deficit
  dat$CMD.def <- 500 - (dat$PPT.dormant)  # start of growing season deficit original value was 400 but 500 seems better
  dat$CMD.def[dat$CMD.def < 0] <- 0  #negative values set to zero = no deficit
  dat$CMDMax <- dat$CMD07
  dat$CMD.total <- dat$CMD.def + dat$CMD
  dat$CMD.grow <- dat$CMD05 + dat$CMD06 +dat$CMD07 +dat$CMD08 +dat$CMD09
  dat$DD5.grow <- dat$DD5_05 + dat$DD5_06 + dat$DD5_07 + dat$DD5_08 + dat$DD5_09
  dat$CMDMax <- dat$CMD07 # add in so not removed below
  dat$DDgood <- dat$DD5 - dat$DD18
  dat$DDnew <- (dat$DD5_05 + dat$DD5_06 +dat$DD5_07  + dat$DD5_08)  - (dat$DD18_05 + dat$DD18_06 +dat$DD18_07 +dat$DD18_08)
  dat$TmaxJuly <- dat$Tmax07
  dat$tmaxSum20 <-  dat %>%
      select( starts_with("Tmax")) %>% -20 %>%
              mutate_all(., list(~ifelse(. < 0, 0 , .))) %>%
                    rowSums(na.rm = TRUE)

dat$tmaxSum25 <-  dat %>%
      select( starts_with("Tmax")) %>% -25 %>%
              mutate_all(., list(~ifelse(. < 0, 0 , .))) %>%
                    rowSums(na.rm = TRUE)

dat$tmaxSum30 <-  dat %>%
      select( starts_with("Tmax")) %>% -30 %>%
              mutate_all(., list(~ifelse(. < 0, 0 , .))) %>%
                    rowSums(na.rm = TRUE)

dat$tmaxSum35 <-  dat %>%
      select( starts_with("Tmax")) %>% -35 %>%
              mutate_all(., list(~ifelse(. < 0, 0 , .))) %>%
                    rowSums(na.rm = TRUE)

# remove some redundant variables considered undesireable
month <- c("01", "02", "03", "04", "05", "06","07", "08", "09", "10", "11", "12")
dat <- dat  %>% dplyr::select(-ends_with(month)) %>% #removes all monthly variables
  dplyr::select(-starts_with("Rad"))# %>% ##remove other non-biological variables
  #dplyr::select(-starts_with("RH")) %>%
  #dplyr::select (-contains("DD_0")) %>%
   #dplyr::select  (-contains("DD18")) %>%
   #dplyr::select  (-contains("DD_18"))  %>%
  #dplyr::select( -PPT_sp, -PAS_sm, -PPT_at, -PAS_at, - MAP, -TD, -MAT, -FFP)
  return(dat)
}


X1 <- addVars(X1)
#X1 <- X1 %>% dplyr::select(ID1, BGC, everything())
### Bioclimate variable set from DataBasin
 X1 <- X1 %>% dplyr::select(ID1, BGC,  MWMT, MCMT, MSP, AHM, SHM,  DD5,  DD18, NFFD, bFFP, eFFP, 
         PAS, EMT, EXT, Eref, CMDMax, CMD.total,  Tave_wt, Tave_sm,  PPT_sm,
         FFP,MAT, TD, MAP, DD_18, DD_0, PPT_wt) # this set could be poorMAR, RH,
# X1 <- X1 %>% dplyr::select(ID1, BGC, MWMT, DD5_at, DD5_sm, DD5_sp, Eref_sm, Eref_sp, PPT_sm, MSP, PPT_sp,
#          CMD_sp, CMDMax,CMD.total,SHM, AHM, Tmax_sp, Tmax_sm, DD18, NFFD)
#$BGC <- as.factor(X1$BGC)
 
 
```


# remove undersampled BGCs
```{r remove undersampled}
removeBGCs <- c("duplicate", "Duplicate", "na")
X1 <- X1 %>% filter(!(BGC %in% removeBGCs))

### count training points per BGC
count_tp <- X1 %>% dplyr::count(BGC) %>% filter(n<=15) %>% select(BGC)
count_tp <-  as.character(count_tp$BGC)

X1 <- X1 %>% filter(!BGC %in% count_tp)
# merge parkland into adjacent alpine unit
X1$BGC <- X1$BGC %>% recode( MHdsp_OR = "CMAun_OR", MHRFdmp_OR = "CMAun_OR", MHRFmmp_OR = "CMAun_OR", MHmsp_WA = "CMAun_WA",
                             "MHRFdsp_CA" = "CMAun_CA", "MHdsp_OR" = "CMAun_OR" , "MHRFdmp_OR"= "CMAun_OR" , "MHRFmmp_OR"= "CMAun_OR" , 
                            "MHmsp_WA" ="CMAun_WA" , "MHmmp_WA"= "CMAun_WA" ,"MHdmp_OR"= "CMAun_OR",
                             "ESSFxcp_CO"=  "IMAun_CO" , "ESSFwmp_MT"="IMAun_MT" , "ESSFxkp_UT"= "IMAun_UT" , "ESSFxkp_MT"= "IMAun_MT" , 
                             "ESSFdmp_ID"="IMAun_ID" ,"ESSFdkp_MT"=  "IMAun_MT" ,"ESSFxcp_WA" = "IMAun_WA" ,"ESSFmwp_WA" = "IMAun_WA" ,
                           "ESSFxwp_OR" = "IMAun_OR", "ESSFxxp_WY" = "IMAun_WY", "ESSFxkp_WY" = "IMAun_WY" )
## merge of related units with high confusion in first run
X1$BGC <- X1$BGC %>% recode( CDFmm_WA = "CDFmm")# , MHRFdmp_OR = "CMAun_OR")
#                             , MHRFmmp_OR = "CMAun_OR", MHmsp_WA = "CMAun_WA",
#                              "MHRFdsp_CA" = "CMAun_CA", "MHdsp_OR" = "CMAun_OR" , "MHRFdmp_OR"= "CMAun_OR" , "MHRFmmp_OR"= "CMAun_OR" , 
#                             "MHmsp_WA" ="CMAun_WA" , "MHmmp_WA"= "CMAun_WA" ,
#                              "ESSFxcp_CO"=  "IMAun_CO" , "ESSFwmp_MT"="IMAun_MT" , "ESSFxkp_UT"= "IMAun_UT" , "ESSFxkp_MT"= "IMAun_MT" , 
#                              "ESSFdmp_ID"="IMAun_ID" ,"ESSFdkp_MT"=  "IMAun_MT" ,"ESSFxcp_WA" = "IMAun_WA" ,"ESSFmwp_WA" = "IMAun_WA" ,
#                            "ESSFxwp_OR" = "IMAun_OR" )
X1 <- X1 %>%  na_if(-9999.0) %>% drop_na()

 count_tp <- X1 %>% dplyr::count(BGC)

 BGCs <- sort(unique(X1$BGC))
BGCs
#X1 <- X2
```
The preprocessing function from the caret package was used to identify variables with near-zero variance or correlation >0.90 in the combined data set. These variables were removed leaving a final variable set of 20 variables.
```{r reduce variables, warning=FALSE} 
########Remove near zero variance and highly correlated variables
# X1_no_nzv_pca <- preProcess(X1[,-c(1:2)], method = c( "nzv")) # DROP variables with near zero variance
# X1_no_nzv_pca
# X1 <- dplyr::select(X1, -c(X1_no_nzv_pca$method$remove))
# X1 <- X1 %>%  na_if(-9999.0) %>% drop_na()
# 
# # calculate correlation matrix of variables
# correlationMatrix <- cor(X1[,-c(1:2)])
# # summarize the correlation matrix
# #print(correlationMatrix)
# # find attributes that are highly corrected (ideally >0.75)
# highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.95, verbose = TRUE, names = TRUE) ## review for removal of highly correlated vars
# X1 <- X1 %>% dplyr::select(-c(highlyCorrelated))%>% drop_na(BGC)
# 
# 
# modelvars <- colnames(X1 %>% dplyr::select(-ID1,  -BGC, everything()))
# X1 <- X1 %>% dplyr::select(ID1, BGC, all_of(modelvars))
# 
# # modelvars <- read.csv("./inputs/Final14Var_WNABGCv11.csv")##use model variables from buildrf work
# #X1 <- X1 %>% dplyr::select(ID1, BGC, modelvars$x) %>% drop_na(BGC)
# X1$BGC <- as.factor(X1$BGC)
# X1 <- droplevels(X1)

```


##Remove outliers points
Looks for training points which fall climatically well outside of the normal range by BGC.
Mostly aimed at catching the 10% of FIA plots which have been given an intentionally large offset georeferenced location
May also indicate overly broad BGCs (eg. IMAus)

```{r remove outlier points}

# outs <- function(dat) {
# Q <- quantile(dat$vals, probs = c(.25, .75), na.rm = TRUE)
# iqr <- IQR(dat$vals)
# up <-  Q[2] + 1.5 * iqr # Upper Range
# low <- Q[1] - 1.5 * iqr # Lower Range
# dat$vals <- ifelse(dat$vals > up, NA,
# ifelse(dat$vals < low, NA, dat$vals))
# dat
# }
# XAll <- X1 %>% distinct()
# outlier.var <- c("DD5", "CMD.total", 'PPT_JAS', 'MCMT')
# XAll3 <- pivot_longer (XAll,!c(BGC, ID1), names_to = "vars", values_to = "vals") %>% unite(BGCvar, c("BGC", "vars"), remove = FALSE) %>% distinct()
# XAll3.1 <- XAll3 %>% filter(!vars %in% outlier.var)%>% distinct()
# XAll3.2 <- XAll3 %>% filter(vars %in% outlier.var)%>% distinct()
# Xreduce <- as.data.frame(XAll3.2)  %>% group_by(BGCvar) %>%  do(outs(.))
# Xreduce <- as.data.frame(Xreduce)  %>% select (ID1, BGC, vars, vals)
# IDs <- XAll3 %>% select(ID1, BGC) %>% distinct(ID1, .keep_all = TRUE)
# #XAll3$BGCvar <- as.factor(XAll3$BGCvar)
# XAll4 <- pivot_wider(Xreduce, id_cols = c(ID1,BGC), names_from = vars, values_from = vals)#, values_fn = length) #%>% distinct()
# XAll5 <- left_join(IDs, XAll4) %>% na_if(NA)
# X2 <- XAll5 %>% na.omit()   %>% select(ID1) %>% left_join(XAll)%>% mutate_if(is.character, as.factor) %>% mutate_if(is.integer, as.numeric)
# X2$ID1 <- as.integer(X2$ID1)# %>% dplyr::select(BGC, AHM, CMD.total, MWMT)
# count_tp <- X2 %>% dplyr::count(BGC)


# X1_Bio1 <- X1_Bio1  %>% select( -BGC) %>% select (Zone, everything())
# # require(psych)
# # require(outliers)
# # X1_test <- X1_Bio1[X1_Bio1$Zone == "CMA",]
# # X1_test <- X1_test[,-c(4:17)]
#  outlier <- psych::outlier(X1_Bio1[-1])
#  X1_Bio1 [,-c(1)] <- dmap (X1_Bio1 [,-c(1)], as.numeric)

#  
# 
removeOutlier <- function(dat, alpha,numIDvars){
  out <- foreach(curr = unique(as.character(dat$BGC)), .combine = rbind) %do% {
    temp <- dat[dat$BGC == curr,]
    md <- tryCatch(mahalanobis(temp[,-c(1:numIDvars)],
                               center = colMeans(temp[,-c(1:numIDvars)]),
                               cov = cov(temp[,-c(1:numIDvars)])), error = function(e) e)
    if(!inherits(md,"error")){
      ctf <- qchisq(1-alpha, df = ncol(temp)-1)
      outl <- which(md > ctf)
      cat("Removing", length(outl), "outliers from",curr, "; ")
      if(length(outl) > 0){
        temp <- temp[-outl,]
      }
    }
    temp

  }
  return(out)
}

 X2 <- removeOutlier(X1, alpha = .025, numIDvars = 2) ###set alpha for removal of outlieres (2.5% = 3SD)

 ####show histogram of variables
 skimmed <- skim(X2)
skimmed



featurePlot(x =  X1[, -c(1:2)],
            y =  X2$Zone,
            plot = "box",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"),
                          y = list(relation="free")))

```
## Reduce sample size of oversampled BGCs via cLHS

```{r downsample over sampled units}
count_tp <- X2  %>% dplyr::count(BGC) %>% filter(n>=2500) %>% select(BGC)
over_sample <-  as.character(count_tp$BGC)

#LHCUnits <- c("CWHdm_OR", "IMAun_WY")
LHCtraining <- foreach(BGC = over_sample, .combine = rbind, .packages = c("clhs", "caret", "dplyr"),.errorhandling="remove") %do% { #
  temp <- X2[(X2$BGC %in% BGC),]
  temp <- temp[,-c(2)]
  temp <- na.omit(temp)
  temp$xx <- ""
  temp <- droplevels(temp)
  Num <- 2500 #X2_LHC$n[(X2_LHC$BGC %in% BGC)]
  nz <- nearZeroVar(temp, names=T) # remove near zero variance variables which cause cLHS to fail
   samples <- clhs(temp[, -which (names(temp) %in% nz)], #CMD with all zeros precluded sampling of coastal units
                  size = Num,           # Test a range of sample sizes
                  iter = 1000,        # Arbitrarily large number of iterations of optimization procedure. Default=10,000 but a larger number may be used
                  progress = FALSE, obj.limit = -Inf, eta = 1,use.cpp = T,
                  simple = FALSE)

  cLHS <- samples$sampled_data

  cLHS <-  cLHS %>% select(ID1)#,BGC)
  #cLHS$ID1 <- row.names(cLHS)
  cLHS_Points <- left_join (cLHS, X2,  by = "ID1")
  cLHS_Points
}

#LHCtraining <- droplevels(LHCtraining)
LHCtraining.list <- unique(as.character(LHCtraining$BGC))
X3 <- X2 %>% filter(!(BGC %in% LHCtraining.list))
X4 <- rbind(X3, LHCtraining)
X2 <- X4 %>% drop_na() %>% filter(!BGC == "") %>% droplevels()
countX <- X2 %>% count(BGC)
```

```{r resample training points}
#Calculate sample number and rescale to semi-balance the training point set
#X2 <- X1#[-1]
#X2 <- X2 %>% filter(BGC != "MHRFmmp_OR", BGC != "ESSFmwp_WA", BGC != "CMAwh")
X2$BGC <- as.factor(X2$BGC)
rownames(X2) <- X2$ID1
countSubzone <- X2 %>% count(BGC)
X2_Few <- countSubzone[countSubzone$n <= 250,]## these will need SMOTE additions

X2_Few$new <- 250
X2_Few <-  X2_Few %>% mutate(ratio = new/n)
X2_Few <- droplevels(X2_Few)
FewUnits <- unique(X2_Few$BGC)

```

 Under trained BGC units are upsampled using the SMOTE to add synthetic points up to the number specified in the resampling.
``` {r add SMOTE units to those BGCs that are undersampled, cache = TRUE}

X2_Smote <- X2[(X2$BGC %in% X2_Few$BGC),]
X2_Smote <- X2_Smote %>% mutate_if(is.integer,as.numeric) #select(-Latitude, -Longitude, -Zone)
X2_Smote$BGC  <- as.factor(X2_Smote$BGC)
X2_Smote$BGCnum <- as.numeric(X2_Smote$BGC)
X2_Smote <- droplevels(X2_Smote)
X2.list <- unique(X2_Smote$BGC)
countSmote <- X2_Smote %>% count(BGC)
smote.num <- X2_Few %>% select(BGC, ratio)
smote.list <- as.character(smote.num$BGC)
#smote.list$BGC <- as.character(smote.list$BGC)
#df <- smote.list %>% mutate(name = paste0(BGC, " = ", ratio))
#BGC = "BGxh1"
# smote.list <- as.pairlist(df$name)
#X2_Smote2 <- SmoteClassif(BGC ~ ., X2_Smote, C.perc = list(BGxh1 = 120),  K = 5 )# creates full balanced data set
# 
#samples <- smotefamily::SMOTE(X2_Smote[,-c(1:2)], X2_Smote$BGCnum , #CMD with all zeros precluded sampling of coastal units
#                 K = 2, dup_size = 2) 
BGC="BGdw_WA"
# ratio
X2_Smote2 <- foreach(BGC = smote.list, .combine = rbind, .packages = c("smotefamily", "caret", "dplyr"),.errorhandling="remove") %do% { #
  temp <- X2_Smote[(X2_Smote$BGC %in% BGC),]
  temp <- temp[,-c(1)]
  temp <- na.omit(temp)
  #temp$xx <- ""
  temp <- droplevels(temp)
  Num <- as.numeric(smote.num$ratio[(smote.num$BGC %in% BGC)])
  #nz <- nearZeroVar(temp, names=T)# remove near zero variance variables which cause cLHS to fail
  #nz <- nz[nz != "BGCnum"]

     #smote.exs <- function(data,tgt,N,k)   
    samples <- smotefamily::SMOTE(temp[-1], temp$BGCnum, #CMD with all zeros precluded sampling of coastal units
                  K = 5, dup_size = Num)# perc.under = 100, k=5,learner = NULL)           # ratio
                         
# 
#  samples <- SMOTE(BGC ~ ., data = temp[, -which (names(temp) %in% nz)], #CMD with all zeros precluded sampling of coastal units
#                  perc.over = Num, perc.under = 100, k=5,learner = NULL)           # ratio
#                          
  smote <- samples$data
  smote$BGC <- BGC
  smote$ID1 <- row.names(smote)
  smote_points <- smote %>% dplyr::select(-BGCnum, -class)
   smote_points <- smote_points %>% dplyr::select(ID1, BGC, everything())
  smote_points
}
count_smote <- X2_Smote2 %>% count(BGC)
X2_2 <- X2[!X2$BGC %in% smote.list,]
X2_final <- rbind (X2_2, X2_Smote2 )
#X2_2 <- rbind (X2_OK2, X2_1)

# X2_OK3 <- X2[(X2$BGC %in% X2_OK$BGC),]
# X2_OK3 <- droplevels(X2_OK3)

# LHC_Final3 <- rbind (LHC_Final2, X2_OK3)
# LHC_Final3 <- droplevels(LHC_Final3)
plyr::count(X2_final, vars = "BGC")
X2_final <- droplevels(X2_final)
countX <- X2_final %>% count(BGC)
```

# Build model of BGCs
The model reduced via RFE to 14 variables is approximately 84.3% accurate and 83% Gini. When all variables are used (44) the accuracy and Gini increase by approximately 1%.
Stay with limited set

```{r build ranger model training point size, cache = TRUE, echo=FALSE}

# set.seed(123321)
# coreNo <- makePSOCKcluster(detectCores() - 1)
# registerDoParallel(coreNo, cores = detectCores() - 1)
# Cores <- as.numeric(detectCores()-1)
# ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 5,
#                      classProbs = FALSE, verboseIter = T,
#                      savePredictions = "final", 
#                      allowParallel = F)
# 
# 
# #BGCmodel_rang <- train(Zone  ~ ., data = X1_Bio1[-2], #for zone
# BGCmodel_subz <- train(BGC  ~ ., data = X1[,-c(1)], # for subzone
#                      method = "ranger")#,
#                      #preProcess = c("center", "scale", "YeoJohnson"), #,tuneGrid = tgrid,
#                       trControl = ctrl,
#                       importance = "impurity")#, splitrule = "extratrees")
# 
#  stopCluster(coreNo)
#  gc()
# BGCmodel_subz
# # predict the outcome on a test set
# BGC_pred <- predict(BGCmodel_subz , X1[,-c(1:2)])
# # compare predicted outcome and true outcome
# con <- confusionMatrix(BGC_pred, as.factor(X1$BGC)) 
# BGCmodel <- BGCmodel_subz
# file=paste("./outputs/USA_SubZone_RFModel_Mar11",".Rdata",sep="")
# save(BGCmodel,file=file)

X2 <- X2_final
X2$BGC <- as.factor(X2$BGC)
#############simple ranger model
modelvars <- as.data.frame(colnames(X2[,-c(1:2)]))
colnames(modelvars) <- "x"
#X2$BGC <- as.factor(X2$BGC)
BGCmodel <- ranger(
  BGC ~ .,
  data = X2[, -c(1)],
  #do.trace = 10,
  num.trees = 501,
  mtry = 5,
  min.node.size = 5,
  importance = "permutation",
  splitrule = "extratrees",
  seed = 12345,
  write.forest = TRUE,
  classification = TRUE
)#strata=BGC, sampsize= c(500),, probability = TRUE

BGCmodel
save(BGCmodel,file= "./outputs/USAv12_26VAR_SubZone_ranger.Rdata")
v <-as.data.frame(BGCmodel$variable.importance)
DF <- v %>% tibble::rownames_to_column() %>% rename(w = rowname, v = 'BGCmodel$variable.importance')
 
 ggplot(DF, aes(x=reorder(w,v), y=v,fill=v))+ 
  geom_bar(stat="identity", position="dodge")+ coord_flip() +
   ylab("Variable Importance")+
   xlab("")+
   ggtitle("Information Value Summary")+
   guides(fill=F)+
   scale_fill_gradient(low="red", high="blue")

 fname <- "USA_12_26var"
model = "_ranger_7Jan2021"
write.csv (BGCmodel$variable.importance, file= paste("./outputs/",fname,"_Importance",model,".csv",sep=""))
write.csv (BGCmodel$prediction.error, file= paste("./outputs/",fname,"_Error",model,".csv",sep=""))
write.csv (BGCmodel$confusion.matrix, file= paste("./outputs/",fname,"_ConfusionMatrix",model,".csv",sep=""))

####alternate model build
#  fname <- "USA_27var"
# model = "_rf_9Oct2020"
# BGCmodel_rf <- randomForest(as.factor(BGC) ~ ., data = X2[,-c(1)], do.trace = 10,
#                           ntree = 101,   importance = TRUE)#strata=BGC, sampsize= c(500),, probability = TRUE
# 
# confusion.matrix <- as.data.frame(BGCmodel_rf$confusion)
# 
# confusion.matrix <- confusion.matrix %>% rownames_to_column(var = "BGC") %>%  
#     select(BGC, class.error, everything()) %>%
#     mutate (nplot = rowSums(.[,-c(1:2)])) %>% 
#     select(BGC, class.error, nplot, everything())
# write.csv (confusion.matrix , file= paste("./outputs/",fname,"_ConfusionMatrix",model,".csv",sep=""), row.names = FALSE)
# #write.csv (BGCmodel_rf$err.rate, file= paste("./outputs/",fname,"_error",model,".csv",sep=""))
```

```{r test overfit}
# trainIndex <- createDataPartition(X2$BGC, p = .7,
#                                   list = FALSE,
#                                   times = 1)
# 
# 
# BGC_train <- XAll[ trainIndex,]
# BGC_train$BGC <- as.factor(BGC_train$BGC)
# BGC_test  <- XAll[-trainIndex,]# %>% droplevels()
# 
# BGCmodel_train <- ranger(BGC ~ ., data = BGC_train[-1],
#                            num.trees = 501,  seed = 12345,
#                             splitrule =  "extratrees", #""gini",
#                             #always.split.variables = c("DD5","CMD.total", "PPT_JAS"), #,
#                             #split.select.weights = var.weight.vec,
#                     mtry = 5,
#                           #max.depth = .5,
#                     min.node.size = 5,
#                            importance = "permutation", write.forest = TRUE, classification = TRUE)
# 
# BGCmodel_train
# 
#  test.pred <- predict(BGCmodel_train, data = BGC_test[,-c(1)])
#   BGC.pred <- as.data.frame(test.pred$predictions)%>% tibble::rownames_to_column() %>% dplyr::rename("BGC.pred" = "test.pred$predictions")
# 
# BGC.test <- BGC_test %>% select(BGC) %>% cbind(BGC.pred) %>%  select( -rowname) %>% mutate_if(is.character, as.factor)
# levels(BGC.test$BGC.pred) <- levels(BGC.test$BGC)
# BGC_accuracy <- BGC.test %>%                   # test set predictions
#   accuracy(truth = BGC, BGC.pred)
#  table(BGC_accuracy)

```
```{r do cross validation}
#####
set.seed(123321)
coreNo <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(coreNo, cores = detectCores() - 1)
Cores <- as.numeric(detectCores()-1)
set.seed(1234)
tr <- trainControl(method = "cv", number = 3, verboseIter = TRUE)
tgrid <- expand.grid(
  .mtry = c(2,3,5),
  .splitrule = "gini",
  .min.node.size = c(5, 10, 20)
)

tr_cv <- train(BGC ~ .,data=BGC_train[-1],method="ranger", trControl= tr, tuneGrid = tgrid, num.trees = 101, num.threads = 7)
tr_cv
tr_cv$bestTune
stopCluster(coreNo)

```



```{r build points file of misclassified point}
###Predict
X2$BGC.pred <- predict(BGCmodel_rf, data = X2[,-c(1:2)])
X2$ID1 <- as.integer(X2$ID1)

X2.mis <- X2 %>% select(ID1, BGC, BGC.pred) %>% filter(BGC != BGC.pred) %>% left_join(X1_loc)

write.csv(X2.mis, "./outputs/Confused_Points_27_Dec2020.csv", row.names = FALSE)

```


```{r graphical confusion for ranger}
# xtab <- BGCmodel_subz$pred
# cm <- confusionMatrix(data = xtab$pred, reference = xtab$obs)
# cm.tab <- as.matrix(cm)
# cm.ovr <- as.matrix(cm,what="overall")
# cm.cls <- as.matrix(cm,what="classes")
# write.csv(cm.cls, "./outputs/SubzoneConfusionStatistics.csv")
# mosaic(cm$table)
# plotCM <- function(cm){
#   cmdf <- as.data.frame(cm[["table"]])
#   cmdf[["color"]] <- ifelse(cmdf[[1]] == cmdf[[2]], "green", "red")
#   
#   alluvial::alluvial(cmdf[,1:2]
#                      , freq = cmdf$Freq
#                      , col = cmdf[["color"]]
#                      , alpha = 0.5
#                      , hide  = cmdf$Freq == 0
#                      )
# }
# plotCM(cm)

```

####Predict BGC membership of hex grid
### Use the best model from those above (Biological, nzv and corr removed, Scaled, Outliers Removed)
``` {r Build }
#load("./outputs/USAv12_62nosmote_VAR_SubZone_ranger.Rdata")
# bring in grid data with climate attributes
region = "WA"
Z1 <- fread(paste0("./inputs/grids/", region, "_400m_HexPts_Normal_1961_1990MSY.csv"), stringsAsFactors = FALSE,  data.table = FALSE)
Z1_loc <-  Z1 %>% dplyr::select(ID1, Latitude, Longitude, Elevation)

Z1 <- addVars(Z1)
#grid.dat <- fread("./inputs/grids/USA_8000m_Grid_Climate_new_Normal_1961_1990MSY.csv", stringsAsFactors = FALSE,  data.table = FALSE)### add in the extra climate variables
#modelvars <- read.csv("./inputs/Final22Var_WNABGCv11.csv")##use model variables from buildrf work
modelvars <- modelvars %>% filter(x !="BGC")
grid.dat <- Z1 %>% dplyr::select(ID1,modelvars$x) #%>%  filter_all(all_vars(. > -9999))

###Predict
grid.pred <- predict(BGCmodel, data = grid.dat[,-c(1)])

Z1_loc <- Z1_loc[Z1_loc$ID1 %in% grid.dat$ID1,]
Z1.pred <- cbind(Z1_loc, grid.pred$predictions)
#Z1.
#grid.dat$Zone <- as.factor(grid.dat$Zone)
Z1.pred$BGC <-  fct_explicit_na(Z1.pred$`grid.pred$predictions` , na_level = "(None)")
#write.csv(grid.dat, "./outputs/Hexgrid800mPredictedtoSubZone_30vars.csv", row.names = FALSE)
Z1.pred <- dplyr::select(Z1.pred, ID1, BGC, Latitude, Longitude,Elevation )#"Longitude","Latitude", "Elevation", "Zone")]
Z1.pred$ID1 <- as.character(Z1.pred$ID1)
#write.csv(Z1.pred, "./outputs/ID_400mPredictedtoSubZone_Jun7.csv", row.names = FALSE)
```
# Attribute hex grid with subzone/variant call
```{r link to hex polygon layer}
require(lwgeom)
##############link predicted Zones to Polygons and write shape file

hexpoly <- st_read(dsn = paste0("./inputs/hex_shapes/", region, "_bgc_hex400.gpkg"))#, layer = "USA_bgc_hex_800m")
hexpoly$hex_id <- as.character(hexpoly$hex_id)
hexZone <- left_join(hexpoly, Z1.pred, by = c("hex_id" = "ID1"))# %>% st_transform(3005) %>% st_cast()
temp <- hexZone %>% select(BGC, geom)
temp2 <- st_zm(temp, drop=T, what='ZM') 
##unremark for undissolved layer
#st_write(temp2, dsn = "./outputs/MT_SubZoneMap_400m_undissolved_7Oct2020.gpkg", driver = "GPKG")

###Dissolve 
#hexZone <- st_read(dsn = "./outputs/WA_bgc_hex8000_ungrouped.gpkg")#, layer = "USA_bgc_hex_800m") ## need to read it back in to ensure all type Polygon is consistent
temp3 <- hexZone
temp3$BGC <- droplevels(temp3$BGC)
temp3 <-  st_as_sf(temp3)# 
st_precision(temp3) <- .5 
temp3$BGC <- forcats::fct_explicit_na(temp3$BGC,na_level = "(None)")
temp3 <- temp3[,c("BGC","Elevation","geom")]
t2 <- aggregate(temp3[,-1], by = list(temp3$BGC), do_union = T, FUN = mean) %>% rename(BGC = Group.1)

wna_boundary = st_read("D:/CommonTables/BC_AB_US_Shp/WNA_State_Boundaries.gpkg") %>% st_as_sf() %>% filter(State %in% region) %>%
  st_transform( crs = st_crs(3005)) %>%
  st_buffer(., dist = 0)# %>%
 # as(., "Spatial")

t2 <- st_zm(t2, drop=T, what='ZM') %>% st_transform(crs = st_crs(3005))  %>% st_buffer(0)
t2 <- st_intersection(t2, wna_boundary)
#mapView(t2)
#CRS.102008 <- "+proj=aea +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m no_defs"
#CRS.102218 <- "+proj=aea +lat_1=43 +lat_2=48 +lat_0=34 +lon_0=-120 +x_0=600000 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"


#t3 <- st_transform_proj(t3, CRS.102218)
#st_is_valid(t3)
#t4 <- st_make_valid(t3)## this seems to remove rather than fix problem areas
 st_write(t2, dsn = paste0("./outputs/", region, "_SubZoneMap_hex400_dissolved_7Jan2021_clipped2.gpkg"), driver = "GPKG", delete_dsn = TRUE)

```
## clean up isolated individual pixels
This process is slow. IS probably better done in QGIS.
In QGIS
Open the export file which is a multipolygon and separate multipart to single parts, (v fast),  add geometric attributes to polygons(v fast), select small polygons by < X area,  select polygons <600 000 = four 400m hexes in size (v fast), eliminate selected polygons by longest neighbour, re dissolve by BGC with SAGA.
Next Step to Polygonize raster map and eliminate small polygons



```{r clean crumbs}
###now cleanup and remove crumbs
tic()
region <- "WA"
t2 <- st_read(dsn = paste0("./outputs/", region, "_SubZoneMap_hex400_dissolved_7Jan2021_clipped.gpkg"))



t2 <- st_cast(t2, "MULTIPOLYGON") %>% st_cast("POLYGON")
t2 <- t2 %>%
  mutate(Area = st_area(.)) %>%
  mutate(ID = seq_along(BGC))
#unique(t2$Area)


library(units)
size <- 600000
size <- set_units(size, "m^2")
tSmall <- t2[t2$Area <= size,]
t2$BGC <- as.character(t2$BGC)

require(doParallel)
coreNum <- as.numeric(detectCores()-1)
coreNo <- makeCluster(coreNum)
registerDoParallel(coreNo, cores = coreNum)

###loop through each polygon < size, determine intersects, and assign to zone with most edge touching
###all the built in functions Kiri found only dealt with holes in the middle of polygons
new <- foreach(i = 1:length(tSmall$ID), .combine = rbind, .packages = c("foreach","sf")) %dopar% {
  ID <- tSmall$ID[i]
  nbrs <- st_intersects(tSmall[i,],t2)[[1]]
  nbrs <- nbrs[!nbrs %in% ID]
  if(length(nbrs) == 0){return(NULL)}
  lines <- st_intersection(t2[ID,],t2[nbrs,])
  lines <- st_cast(lines)
  l.len <- st_length(lines)
  names(l.len) <- lines$BGC.1
  zn <- names(l.len)[l.len == max(l.len)][1]
  newDat <- t2[ID,]
  newDat$BGC <- zn
  newDat
}
#
stopCluster(coreNo)
gc()
temp <- t2[!t2$ID %in% new$ID,]
t2 <- rbind(temp, new) %>%
  mutate(BGC = as.factor(BGC))
#
# ###now have to combine crumbs with existing large polygons
temp2 <- t2
st_precision(temp2) <- 0.5
t2 <- temp2 %>%
  group_by(BGC) %>%
  summarise(geom = sf::st_union(geom)) %>%
  ungroup()
#
#mapview(t2, zcol = "BGC")
t2 <- st_zm(t2, drop=T, what='ZM')
st_write(t2, dsn = paste0("./outputs/", region, "_BGC_1961_1991_7Jan2021_eliminated.gpkg"), driver = "GPKG",delete_dsn = TRUE)
toc()
# tic()
# t2_smooth <- smooth(t2, method = "ksmooth", smoothness = 2)
# st_write(t2_smooth, dsn = paste0("./outputs/", region, "_BGC_1961_1991_smoothed3.gpkg"), driver = "GPKG",delete_dsn = TRUE)
# toc()
```


# Build Subzone models and predict within each Zone
####now build models for each subzone and then predict gridpoints by subzone model
Advice of Tom Hengel is that single model is more mathematically justified

``` {r Build Subzone model and predict}

# # Grid data from previous chunk
# pointDat <- grid.dat
# pointDat <-  pointDat %>% select(Zone, everything()) %>% arrange(Zone)
# 
# ######################################################
# #Training point data from above
# X2 <-  X1.info %>% select(Zone, everything()) %>% arrange(Zone)
# X2$BGC <-  as.factor(X2$BGC)
# Zones <- as.character(unique(X2$Zone))
# Zones <- sort (Zones)
# X2 <- droplevels(X2)
# # X2 <- X2[X2$Zone == "CMA",]
# # Zones <- as.character(unique(X2$Zone))
# # Zones <- sort (Zones)
# SZPred <- foreach(Z = Zones, .combine = rbind) %do% {
#   trainSub <- X2[X2$Zone == Z,]###subset training points to only include selected zone
#    if(length(unique(trainSub$BGC)) >= 2){ ###somtimes there aren't any subzones skip to else
#     trainSub$BGC <- as.factor(trainSub$BGC)
#     trainSub$BGC <- droplevels(trainSub$BGC)
#     trainSub <- removeOutlier(trainSub, alpha = 0.001)
# ###build model for each subzone individually
#     set.seed(123321)
# coreNo <- makePSOCKcluster(detectCores() - 1)
# registerDoParallel(coreNo, cores = detectCores() - 1)
# Cores <- as.numeric(detectCores()-1)
# 
# SZmodel <- train(BGC  ~ ., data = trainSub [-1],
#                      method = "ranger",
#                      trControl = trainControl(method="cv", number = 10, verboseIter = T, classProbs = T, savePredictions = "final"),
#                      #num.trees = 11,
#                      preProcess = c("center", "scale", "YeoJohnson"),#,tuneGrid = tgrid,
#                      importance = "impurity")
# 
#  stopCluster(coreNo)
# gc()
#       # SZmodel <- randomForest(BGC ~ ., data=trainSub [-1], nodesize = 5, do.trace = 10,
#       #                       ntree=101, na.action=na.fail, importance=TRUE, proximity=FALSE)
# 
#    
#     pointSub <- pointDat[pointDat$Zone == Z,] ###subset grid based on previously predicted zones
#     pointSub$BGC<- predict(SZmodel, newdata = pointSub[,-c(1:4),]) ### predict subzones
#     out <- pointSub[,c("ID1", "Latitude","Longitude","Zone", "BGC")]
#     out
#   }else{ ##if only one subzone, plot
#     pointSub <- pointDat[pointDat$Zone == Z,]
#     #pointSub$Zone <- droplevels(pointSub$Zone)
#     pointSub$BGC <- pointSub$Zone
#     out <- pointSub[,c("ID1", "Latitude","Longitude","Zone", "BGC")]
#     out
#   }
#   
# }
# 
# grid.sbz <- dplyr::select(SZPred, ID1, BGC, Zone)#"Longitude","Latitude", "Elevation", "Zone")]
# grid.sbz <- droplevels(grid.sbz)
# table(grid.sbz$BGC)
# grid.dat.pred <- left_join(grid.dat, grid.sbz, by = c("ID1", "Zone"))
# grid.dat.pred <- grid.dat.pred %>% select(ID1, Zone, BGC, everything())
# write.csv(grid.dat, "./outputs/Hexgrid800mPredictedtoSubZone_ranger.csv", row.names = FALSE)
# ```
# # Attribute hexpolygons with subzone predictions 
# ```{r link subzone predictions to hex polygons}
# ###link to hex polygons
# hexZone2 <- left_join(hexZone, grid.sbz, by = "ID1")
# #temp <- select(hexZone, Zone, geometry)
# temp <- hexZone2
# temp$BGC <-  fct_explicit_na(temp$BGC , na_level = "(None)")
# table(temp$BGC)
# 
# temp2 <- temp
# st_precision(temp2) <- 0.5###union polygons within zsubone
# t3 <- temp2 %>%
#   group_by(BGC) %>%
#   summarise(geometry = sf::st_union(geometry)) %>%
#   ungroup()
# 
# mapview(t3)
# t3 <- st_zm(t3, drop=T, what='ZM')
# st_write(t3, dsn = "outputs", layer = "SubzoneRaw800m_ranger_30", driver = "ESRI Shapefile", update = TRUE)


```

## This should probably be done from the spatial file since it will have be simplified with decrumbing

```{r cLHS of trainingpoints for WNA}
#select a subset of training points by BGC from the final surface for full model build
# library(clhs)
# #BGC = "ICHxwz"
# #countZone <- points %>% count(Zone)
# countSubzone <- grid.dat.pred %>% count(BGC)
# 
# rownames(grid.dat.pred) <- grid.dat.pred[,1]
# countSubzone$logn <- log(countSubzone$n, 10)
# countSubzone$rs <- as.integer (rescale(countSubzone$logn, to = c(500, 1200), from = range(countSubzone$logn, na.rm = TRUE, finite = TRUE)))
# countSubzone$sample <- ifelse(countSubzone$rs > countSubzone$n, countSubzone$n, countSubzone$rs )
# write.csv (countSubzone, "./outputs/USA_pts_per_BECLHC2.csv")
# 
# allUnits <- unique(grid.dat.pred$BGC)
# #grid.dat.pred$ID1 <- row.names(grid.dat.pred)
# set.seed(123321)
# coreNo <- makePSOCKcluster(detectCores() - 1)
# registerDoParallel(coreNo, cores = detectCores() - 1)
# Cores <- as.numeric(detectCores()-1)
# 
# BGC = "BGdh_OR"
# LHCtraining <- foreach(BGC = allUnits, .combine = rbind, .packages = c("clhs")) %dopar% {
# temp <- grid.dat.pred[(grid.dat.pred$BGC %in% BGC),]
# temp_names <- temp[1:6]
# Num <- countSubzone$sample[(countSubzone$BGC %in% BGC)]
#   samples <- clhs(temp[,-c(1:6)], 
#                 size = Num,           # Test a range of sample sizes
#                 iter = 10000,        # Arbitrarily large number of iterations of optimization procedure. Default=10,000 but a larger number may be used
#                 progress = TRUE, 
#                 simple = FALSE)
# 
# cLHS <- samples$sampled_data
# cLHS$ID1 <- row.names (cLHS)
# cLHS_Points <- merge (temp[,c(1:5)], cLHS, by = "ID1")
# cLHS_Points
# }
#  stopCluster(coreNo)
#  gc()
# 
# write.csv(LHCtraining, "./outputs/USA_Training_LHC_w_data.csv", row.names = FALSE)
# LHCtraining2 <- LHCtraining [1:3] 
# rownames(grid.dat.raw) <- grid.dat.raw[,1]
# LHCtraining2 <- left_join(LHCtraining2, grid.dat.raw, by = "ID1")
# write.csv(LHCtraining2, "./outputs/USA_LHS_all_dat.csv", row.names = FALSE)
# USA_LHS <- dplyr::select(LHCtraining2, ID1, BGC, Longitude, Latitude, Elevation)
# write.csv(USA_LHS, "./outputs/USA_LHS_for_ClimateNA.csv", row.names = FALSE)
#X1 <- LHCtraining #feed these back into building a new rF model above
```
