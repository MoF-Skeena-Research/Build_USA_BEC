---
title: "Model USA Biogeoclimatic Units using Machine Learning"
author: "William H MacKenzie and Kiri Daust"
date: "17/10/2019"
output: html_document
---
# Script to create RF model from USA training points and predict + map US zones/ subzones (predicted within each zone)
Import training data
Reduce variable space with caret or PCA all variable
Build machine learning model
Test missing variable space or map error for required additional training points
Loop to build models by zone and then by subzone within zone to predict map
Translates predictions to a grid point map
Add predictions to hex polygon layer.


# STEP 1: Prepare dataset for analysis----clear workspace
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(rattle)
require(rpart)
require(rpart.plot)
require(plyr)
require(reshape)
require(reshape2)
#require(VSURF)
#require(reports)
#require(rpart.utils)
#require(rfUtilities)
require(parallel)
require(foreach)
require(doParallel)
require(ggplot2)
#require(functional)
require(plot3D)
#require(dplyr)
require(tcltk)
require(caret)
require(randomForest)
require(ranger)
#require (OpenMP)
#require (randomForestSRC)
require (tools)
#require(data.table)
require(spatstat)
require(spatialEco)
require(survey)
require(scales)
require(UBL)
require (mcl3)
require (tidyverse)
```

# Import training point data 
CSV file with all variable climate data generated by ClimateNA for the 1960-91 Normal Period
```{r input data, echo=FALSE}

fplot=(file.choose()) 
X1 <- fread(fplot,  stringsAsFactors = FALSE,data.table = FALSE)#, select = Columns
#temporary script to rename and simplify training data set
# X1 <-  X1[,c(1, 4:7)]
# #X1$ZoneSubzone <- str_replace_all(X1$ZoneSubzone, fixed(" "), "")
# X2 <- X1 %>%rename("Old_BGC" = "ZoneSubzone") %>%
#    left_join(X1name, by = "Old_BGC") %>%
#   dplyr::select("ID", "BGC", "LAT", "LON", "ELEV_m")
# write.csv(X2, "USA_Training_renamed.csv", row.names = FALSE)

## add some additional calculated variables
X1$PPT_MJ <- X1$PPT05 + X1$PPT06 # MaY/June precip
X1$PPT_JAS <- X1$PPT07 + X1$PPT08 + X1$PPT09 # July/Aug/Sept precip
X1$PPT.dormant <- X1$PPT_at + X1$PPT_wt # for calculating spring deficit
X1$CMD.def <- 500 - (X1$PPT.dormant)# start of growing season deficit original value was 400 but 500 seems better
X1$CMD.def [X1$CMD.def < 0] <- 0 #negative values set to zero = no deficit
X1$CMDMax <- X1$CMD07
X1$CMD.total <- X1$CMD.def + X1$CMD
X1save = X1
X1 = X1save

```

###############Build RandomForest Model t#################

```{r reduce variable set, echo=FALSE}
#VarList = c("CMD.total", "bFFP","Eref_sm","MCMT","PPT_JAS",
 #           "SHM","TD","PAS","DD5_sp","PPT06")
############New 16 Variable Set to Use 2019
########test value of Tmin02 versus MCMT and replace if better.
VarList = c("TD",	"Eref",	"AHM",	"Tmax07",	"CMD.total",	"Tmax_sp","Tmin_sm","CMD_sp","PAS",	
            "PPT_sp","PPT06","CMD.def","DD5_sp","Tmin_at","bFFP","MCMT")
#List = c("rn", "Region", "Latitude", "Longitude", "Elevation")
List = c("ID", "BGC", "Latitude", "Longitude", "Elevation")
X1.sub = X1
X1.sub=X1[,names(X1) %in% c(List,VarList)]
X1 <- X1.sub
###########################
#X1 <- X1[!is.na(X1$Latitude),]
####modify variable names
Y1 <- X1
colnames(X1)[1]=c("PlotNo")
colnames(X1)[2]=c("BGC")
records <- nrow(X1)
X1$PlotNo <- as.character (X1$PlotNo)
#attr(X1, "row.names") <- (X1$PlotNo)
X2 <- X1 [, c("PlotNo", "BGC", "Latitude", "Longitude", "Elevation")]
X1 <- X1[,-c(1,3:5)] #c("PlotNo","Latitude", "Longitude", "Elevation")]
####for the lHC training data from below.
# X2 <- X1 [, c("ID1", "Zone", "Subzone", "Latitude", "Longitude", "Elevation")]
# drop <- c("ID1", "Zone", "Latitude", "Longitude", "Elevation")
# X1= X1 [, !(names(X1) %in% drop)]
# colnames (X1) [1] = "BGC" 
# X1save2 = X1
#X1<- as.data.frame(X1)
#X1$BGC <- gsub("[[:space:]]","",X1$BGC) ## for subzone
############## Drop subzone for zone model
#X1$BGC <- gsub("[[:lower:]]|[[:space:]]|[[:digit:]]", "", X1$BGC) ###for zone model
X1 <-  drop_na (X1)
X1$BGC <- as.factor(X1$BGC)
X1 <- droplevels(X1)
#colnames(X1)[1]<- "Zone"
##########to be tested - training point balancing
X1.sub2 <- SmoteClassif(BGC ~ ., X1, C.perc = "balance", k= 5 , repl = TRUE, dist = "Euclidean")
prop.table(table(X1.sub2$BGC))
prop.table(table(X1$BGC))

```


##Build machine learning model

```{r plots of variables}

require(doParallel)
set.seed(123321)
coreNo <- makeCluster(detectCores() - 1)
registerDoParallel(coreNo, cores = detectCores() - 1)
Cores <- as.numeric(detectCores()-1)
##may need to change BGC for Zone where running for entire model of for subzone by zone foreach loop
ptm <- proc.time()
BGCmodel <- ranger(BGC ~ ., data=X1, verbose = TRUE) #nodesize = 5, do.trace = 10,
                        # num.trees=71, importance="impurity", )
proc.time() - ptm
stopCluster(coreNo)
##############Parallel version
# ptm <- proc.time()
# 
# Zonemodel <- foreach(ntree = rep(15,7), .combine = randomForest::combine, .multicombine = TRUE, .packages = "randomForest") %dopar% {
#   randomForest(X1[-1], X1[1],
#                ntree=ntree, na.action=na.fail, importance=TRUE, proximity=FALSE)
# }
# proc.time() - ptm
   #### Save random forest model
file=paste("USA_Subzones_RFModel_16Var_20Nov2019",".Rdata",sep="")
save(BGCmodel,file=file)
#load("USA_Subzones_RFModel_16Var_20Nov2019.Rdata")
#load("USA_Subzones_RFModel_27Var.Rdata")
#load(file.choose())

```



#####Predict BGC units##########################
```{r predict BGC for hexgrid}
#load("BGCv11_AB_USA_LHC_10VAR_SubZone_RFmodel.Rdata") ##load random forest model
### Read in US Hex Grid file with normal data. SOMETIMES THE FIRST 2 COLUMN NAMES NEED TO BE CHANGED
Columns = c("rn", "Region", "Latitude", "Longitude", "Elevation", "AHM", "bFFP",
            "CMD07","DD5_sp","EMT","Eref_sm","EXT","FFP","MCMT","MSP",
            "PPT07","PPT08", "PPT05","PPT06","PPT09", "SHM","TD","Tmax_sp","Tmin_at",
            "Tmin_sm","Tmin_wt", "PPT_at","PPT_wt", "PAS","eFFP",
            "Eref09","MAT","Tmin_sp","CMD")#"GCM", 

pointDat <- fread(file.choose(), stringsAsFactors = FALSE,  data.table = FALSE)#, select = Columns
#pointDat <- pointDat[pointDat$DD_0_01 != -9999,]
#pointDat <- pointDat[pointDat$Latitude < 49,]
X1 <-pointDat
### add additional variables to match rF model
X1$PPT_MJ <- X1$PPT05 + X1$PPT06 # MaY/June precip
X1$PPT_JAS <- X1$PPT07 + X1$PPT08 + X1$PPT09 # July/Aug/Sept precip
X1$PPT.dormant <- X1$PPT_at + X1$PPT_wt # for calculating spring deficit
X1$CMD.def <- 500 - (X1$PPT.dormant)# start of growing season deficit original value was 400 but 500 seems better
X1$CMD.def [X1$CMD.def < 0] <- 0 #negative values set to zero = no deficit
X1$CMDMax <- X1$CMD07
X1$CMD.total <- X1$CMD.def + X1$CMD
X1save = X1
X1 = X1save
colnames(X1)[1]=c("ID1")
colnames(X1)[2]=c("ID2")
#####New 16 Variable list to Use.
VarList = c("TD",	"Eref",	"AHM",	"Tmax07",	"CMD.total",	"Tmax_sp","Tmin_sm","CMD_sp","PAS",	
            "PPT_sp","PPT06","CMD.def","DD5_sp","Tmin_at","bFFP","MCMT")#16Var
#VarList = c("AHM", "bFFP","CMD.total","DD5_sp","EMT","Eref_sm","EXT","FFP","MCMT","MSP",
#            "PPT_JAS","PPT_MJ","PPT06","SHM","TD","Tmax_sp","Tmin_at","Tmin_sm","Tmin_wt",
#            "PAS","CMD.def","CMDMax","eFFP","Eref09","MAT","PPT07","Tmin_sp")
List = c("ID1","ID2", "Latitude", "Longitude", "Elevation")
X1.sub = X1
X1.sub=X1[,names(X1) %in% c(List,VarList)]
X1.sub <- X1.sub[!X1.sub[,6] == -9999.0,]
X1 <- X1.sub
pointDat <-X1.sub
#pointDat <- Y1
###Predict BGC units
ranger.pred <- predict(BGCmodel, data = pointDat[,-c(1:5)])
pointDat$BGC.pred <- ranger.pred$predictions
 pointDat.saved <- pointDat
 pointDat <-pointDat.saved
 #points <- pointDat[,c("Latitude","Longitude","Zone")]
 pointDat$BGC.pred <- as.character(pointDat$BGC.pred)
 pointDat$Subzone <- gsub("[[:space:]]","",pointDat$BGC.pred)##for subzones
  pointDat$Zone <- word(pointDat$BGC.pred,1,sep = "\\_")
 pointDat$Zone <- gsub("[[:lower:]]|[[:space:]]|[[:digit:]]","",pointDat$Zone)##for just zone
points <- pointDat[,c("ID1", "Zone", "Subzone", "Latitude","Longitude","Elevation")]
write.csv (points, "TestPredictedPoints.csv")
List2 = c("ID1", "Zone", "Subzone", "Latitude","Longitude","Elevation")
#List2 = c("ID1", "Subzone")
pointLHC <- pointDat[,names(pointDat) %in% c(List2,VarList)]###includes all the climate variables for LHC procedure below
colnames (pointLHC) [18] <- "ID2"
col_order <-c("ID1", "Zone", "Subzone", "Latitude","Longitude","Elevation","CMD.total", "bFFP","Eref_sm","MCMT","PPT_JAS",
              "SHM","TD","PAS","DD5_sp","PPT06")
pointLHC <- pointLHC[,col_order]
#colnames(points)[2]<- "ZoneSubzone"
#colnames(points)[1]<- "ID1"
```

###Write to CSV to import into QGIS and join to HexGrid

``` {r write predicted point data to CSV}
write.csv (points, "USA_FAIPlots_BGCPredicted.csv")
write.csv (pointLHC, "USA_1k_HexGridPts_BGC_predicted_16Var_LHC_May3_w_BGCdata.csv")
countZone <- points %>% count(Zone)
countSubzone <- points %>% count(Subzone)
countSubzone$logn <- log(countSubzone$n, 10)
countSubzone$rs <- as.integer (rescale(countSubzone$logn, to = c(300, 1000), from = range(countSubzone$logn, na.rm = TRUE, finite = TRUE)))
countSubzone$sample <- ifelse(countSubzone$rs > countSubzone$n, countSubzone$n, countSubzone$rs )
allUnits <- unique(points$Subzone)
write.csv (countSubzone, "USA_ptPerBECLHC2.csv")

###########From here add CSV to QGIS and Join by ID1 to HexPolygon Shape file
```  

##############select a subset of training points   
``` {r select a subsample of training points from each predicted BGC for use in WNA model}
library(clhs)
BGC = "ICHxwz"
allUnits <- unique(pointLHC$Subzone)
pointLHC$ID1 <- row.names(pointLHC)
LHCtraining <- foreach(BGC = allUnits, .combine = rbind, .packages = c("clhs")) %dopar% {
temp <- pointLHC[(pointLHC$Subzone %in% BGC),]
Num <- countSubzone$sample[(countSubzone$Subzone %in% BGC)]
  samples <- clhs(temp[,-c(1:6)], 
                size = Num,           # Test a range of sample sizes
                iter = 100,        # Arbitrarily large number of iterations of optimization procedure. Default=10,000 but a larger number may be used
                progress = TRUE, 
                simple = FALSE)

cLHS <- samples$sampled_data
cLHS$ID1 <- row.names (cLHS)
cLHS_Points <- merge (pointLHC[,c(1:6)], cLHS, by = "ID1")
cLHS_Points
}

write.csv(LHCtraining, "USA_Training_LHC.csv")
X1 <- LHCtraining #feed these back into building a new rF model above


