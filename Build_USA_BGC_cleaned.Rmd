---
title: "Model USA Biogeoclimatic Units using Machine Learning"
author: "William H MacKenzie and Kiri Daust"
date: "17/10/2019"
output: html_document
---
# Script to create RF model from USA training points and predict + map US zones/ subzones (predicted within each zone)
Import training data
Reduce variable list with caret or PCA all variable
Build machine learning model
Test for missing variable space or map error for required additional training points
Loop to build models by zone and then by subzone within zone to predict map
Translates predictions to a grid point map
Add predictions to hex polygon layer.
Overlay additional plots over map to assign to USA_BGC units


# STEP 1: Prepare dataset for analysis----clear workspace
```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(rattle)
require(rpart)
require(rpart.plot)
require(plyr)
require(reshape)
require(reshape2)
#require(VSURF)
#require(reports)
#require(rpart.utils)
#require(rfUtilities)
require(parallel)
require(foreach)
require(doParallel)
require(ggplot2)
#require(functional)
require(plot3D)
require(dplyr)
require(tcltk)
require(caret)
require(randomForest)
require(ranger)
#require (OpenMP)
#require (randomForestSRC)
require (tools)
require(data.table)
require(spatstat)
require(spatialEco)
require(survey)
require(scales)
require(UBL)
#require (mcl3)
require(tidyr)
require (tidyverse)
require(rlang)
require (Rcpp)
require(sf)
require (mapview)
require(forcats)
require(Boruta)
require(purrrlyr)
require(skimr)
require(gbm)
require(vcd)
require(alluvial)
#install.packages ("alluvial")
```

# Import USA training point data 
CSV file with all variable climate data generated by ClimateNA for the 1960-91 Normal Period.
Several additional climate variables are calculated
```{r input data, echo=FALSE}

X1 <- fread("./inputs/training_pts/US_TrainingPointsAll_30Dec2019_Normal_1961_1990MSY.csv",  stringsAsFactors = FALSE,data.table = FALSE)
## add some additional calculated variables
X1$PPT_MJ <- X1$PPT05 + X1$PPT06 # MaY/June precip
X1$PPT_JAS <- X1$PPT07 + X1$PPT08 + X1$PPT09 # July/Aug/Sept precip
X1$PPT.dormant <- X1$PPT_at + X1$PPT_wt # for calculating spring deficit
X1$CMD.def <- 500 - (X1$PPT.dormant)# start of growing season deficit original value was 400 but 500 seems better
X1$CMD.def [X1$CMD.def < 0] <- 0 #negative values set to zero = no deficit
X1$CMDMax <- X1$CMD07
X1$CMD.grow <- X1$CMD05 + X1$CMD06 +X1$CMD07 +X1$CMD08 +X1$CMD09
X1$CMD.total <- X1$CMD.def + X1$CMD
rdc <- function(x, na.rm = FALSE){
  (x - 25)}
zero <- function (x){
    any(x<=0)}
setzero <- function(x){
    if_else(x<0,0,x)}
X1$heatsum <-  X1 %>% select(starts_with("Tmax")) %>% select(-starts_with("Tmax_")) %>% mutate_all(rdc) %>% mutate_if(zero, setzero) %>% rowSums(na.rm=TRUE)

## split BGC into Zone and subzone and remove NA
left = function(text, num_char) {
  substr(text, 1, num_char)
}
X1$Zone <- left(X1$BGC, 4)
X1$Zone <- gsub("[[:lower:]]|[[:space:]]|[[:digit:]]", "", X1$Zone) ###for zone model
X1 <- X1[X1$Zone != "",]
X1$Zone <- as.factor(X1$Zone)
X1 <- droplevels(X1)
X1$BGC <- gsub("  ","",X1$BGC) ## for subzone
X1$BGC <- gsub(" ","",X1$BGC) ## for subzone
X1_Bio <- X1
X1_Bio <- X1_Bio  %>% select(ID, Zone, BGC, everything()) %>% # select (-Latitude, -Longitude, -Elevation,) %>% 
  select(-starts_with("Rad")) %>% select(-contains("0"))%>% select(-contains("1")) #removes all monthly variables


```
# Various tests to reduce the covariate set
``` {r previous biologically interpretable only}
# library(caret)
# 
# #### BioAll model - All Biologically Interpretable Variables Only:
# 
# TEMP.list=c("Tmax_sp","Tmax_sm", "Tmax_wt", "Tmin_wt","Tmin_sp","Tmin_sm","Tmin_at",
#             "DD5_sm","DD5_at","DD5", "Tmax07","Tmax08",
#             "MAT","MWMT","MCMT", "EXT", "DD5_06", "DD5_05", "TD", "Tmin01") #"DD5_sp","EMT",
# PPT.list=c( "MSP", "MAP","PAS", "PPT_MJ", "PPT_JAS", "PPT.dormant", "PPT_at" )#"PPT_sp", "PPT_sm","PPT06",
# OTHER.list=c("CMD_sp","CMD_sm","AHM","SHM","NFFD","bFFP","FFP","NFFD_sp",  "CMD", "CMD07",  "Eref","CMD.total", "heatsum", "MAR")#CMD.grow"
# ClimateVar=c(TEMP.list,PPT.list,OTHER.list)
# List=c("Zone", "BGC")
# X1_Bio <- X1[,names(X1) %in% c(List,ClimateVar)]
# X1_Bio$BGC <- as.factor(X1_Bio$BGC)
# X1_Bio <-X1_Bio  %>% select(Zone, BGC, everything())
# X1_Bio1 <- X1_Bio

```

``` {r Feature Reduction}
#Feature Selection by Boruta. All of the biologiclly important variables are retained by Boruta
    # bor <- Boruta(BGC ~ ., X1_Bio[-1], doTrace = 2)
    #  print(bor)
    #  boruta.bank <- TentativeRoughFix(bor)
    #  plot(boruta.bank)
    #  borvar.list <- getSelectedAttributes(boruta.bank, withTentative = F)
#try using Vita package -- much faster but fails due to no negative importance values
# hrf <- holdoutRF(BGC ~ ., X1_Bio[-44])
# importance_pvalues(hrf, method = "janitza")

########Remove near zero variance and highly correlated variables
      X1_Bio_no_nzv_pca <- preProcess(select(X1_Bio, - c(ID, Zone, BGC  ,Latitude, Longitude, Elevation)),
                              method = c("nzv", "corr"), cutoff = .95)

      X1_Bio_no_nzv_pca$method$remove
      X1_Bio1 <- select(X1_Bio, -c(X1_Bio_no_nzv_pca$method$remove))
      X1_Bio1 <- select(X1_Bio1, -MAR)
      X1_Bio1 <-  drop_na (X1_Bio1)
      #X1_Bio1 %>% replace_with_na(replace = list(x = -9999.0))
      X1.info <- X1_Bio1 %>%  select(ID, Zone, BGC, Latitude, Longitude, Elevation, everything())
      X1_Bio1 <- X1_Bio1 %>% select(-ID, -Latitude, -Longitude, -Elevation)
      X1_Bio1$Zone <- as.factor(X1_Bio1$Zone)
      X1_Bio1$BGC <- as.factor(X1_Bio1$BGC)
      X1_Bio1 <- droplevels(X1_Bio1)
      X1.var <- colnames(X1_Bio1)
      X1.var

```
##Remove outliers points
Looks for training points which fall climatically well outside of the normal range by BGC.
Mostly aimed at catching the 10% of FIA plots which have been given an intentionally large offset georeferenced location
May also indicate overly broad BGCs (eg. IMAus)

```{r remove outlier points}

# X1_Bio1 <- X1_Bio1  %>% select( -BGC) %>% select (Zone, everything())
# # require(psych)
# # require(outliers)
# # X1_test <- X1_Bio1[X1_Bio1$Zone == "CMA",]
# # X1_test <- X1_test[,-c(4:17)]
#  outlier <- psych::outlier(X1_Bio1[-1])
#  X1_Bio1 [,-c(1)] <- dmap (X1_Bio1 [,-c(1)], as.numeric)
# md <- mahalanobis(X1_Bio1[,-1], center = colMeans(X1_Bio1[,-1]), cov = cov(X1_Bio[,-1]))
# 
# alpha <- .001
# 
# cutoff <- (qchisq(p = 1 - alpha, df = ncol(data_outlier)))
# 
# names_outliers_MH <- which(md > cutoff)
# 
# excluded_mh <- names_outliers_MH
# 
# data_clean_mh <- data_outlier[-excluded_mh, ]
# 
# data[excluded_mh, ] 
#  
 
# plot(outlier)

removeOutlier <- function(dat, alpha){
  out <- foreach(curr = unique(as.character(dat$Zone)), .combine = rbind) %do% {
    temp <- dat[dat$Zone == curr,]
    md <- tryCatch(mahalanobis(temp[,-c(1:2)],center = colMeans(temp[,-c(1:2)], cov = cov(temp[,-c(1:2)]))),
                   error = function(e) e
    )
    if(!inherits(md,"error")){
      ctf <- qchisq(1-alpha, df = ncol(temp)-1)
      outl <- which(md > ctf)
      cat("Removing", length(outl), "outliers from",curr, "; ")
      if(length(outl) > 0){
        temp <- temp[-outl,]
      }
    }
    temp
    
  }
  return(out)
}

 X1_Bio2 <- removeOutlier(X1_Bio1, alpha = .001)

 ####show histogram of variables
 skimmed <- skim(X1_Bio2)
skimmed



featurePlot(x =  X1_Bio1[, 3:31], 
            y =  X1_Bio2$Zone, 
            plot = "box",
            strip=strip.custom(par.strip.text=list(cex=.7)),
            scales = list(x = list(relation="free"), 
                          y = list(relation="free")))

```

# Build model of BGCs
The model reduced via RFE to 14 variables is approximately 84.3% accurate and 83% Gini. When all variables are used (44) the accuracy and Gini increase by approximately 1%.
Stay with limited set

```{r Build Zone model, echo=FALSE}

# BGCmodel <- randomForest(Zone  ~ ., data = X1_Bio1[-2])
# # 
#  varImpPlot(BGCmodel)
# 

# Test models between all training points and where outliers removed
set.seed(123321)
coreNo <- makePSOCKcluster(detectCores() - 1)
registerDoParallel(coreNo, cores = detectCores() - 1)
Cores <- as.numeric(detectCores()-1)
ctrl <- trainControl(method = "cv", number = 3, #repeats = 3,
                     classProbs = TRUE, verboseIter = T,
                     savePredictions = "final", 
                     allowParallel = T)


#BGCmodel_rang <- train(Zone  ~ ., data = X1_Bio1[-2], #for zone
BGCmodel_subz <- train(BGC  ~ ., data = X1_Bio2[-1], # for subzone
                     method = "ranger",
                     preProcess = c("center", "scale", "YeoJohnson"), #,tuneGrid = tgrid,
                      trControl = ctrl,
                      importance = "impurity")
# BGCmodel_xgb <- train(Zone  ~ ., data = X1_Bio1[-2], #for zone
# #BGCmodel2 <- train(BGC  ~ ., data = X1_Bio2[-1], # for subzone
#                      method = "xgbTree",
#                      preProcess = c("center", "scale", "YeoJohnson"), #,tuneGrid = tgrid,
#                       trControl = ctrl,
#                       importance = "impurity")
# 
# BGCmodel_kknn <- train(Zone  ~ ., data = X1_Bio1[-2], #for zone
# #BGCmodel2 <- train(BGC  ~ ., data = X1_Bio2[-1], # for subzone
#                      method = "kknn",
#                      preProcess = c("center", "scale", "YeoJohnson"), #,tuneGrid = tgrid,
#                       trControl = ctrl,
#                       importance = "impurity")

# BGCmodel_nnet <- train(Zone  ~ ., data = X1_Bio1[-2], #for zone
# #BGCmodel2 <- train(BGC  ~ ., data = X1_Bio2[-1], # for subzone
#                      method = "nnet",
#                      preProcess = c("center", "scale", "YeoJohnson"), #,tuneGrid = tgrid,
#                       trControl = ctrl,
#                       importance = "impurity")

 stopCluster(coreNo)
 gc()

BGCmodel <- BGCmodel_subz
file=paste("./outputs/USA_SubZone_RFModel_Dec30",".Rdata",sep="")
save(BGCmodel,file=file)
```

```{r graphical confusion}
xtab <- BGCmodel_subz$pred
cm <- confusionMatrix(data = xtab$pred, reference = xtab$obs)
cm.tab <- as.matrix(cm)
cm.ovr <- as.matrix(cm,what="overall")
cm.cls <- as.matrix(cm,what="classes")
write.csv(cm.cls, "./outputs/SubzoneConfusionStatistics.csv")
mosaic(cm$table)
plotCM <- function(cm){
  cmdf <- as.data.frame(cm[["table"]])
  cmdf[["color"]] <- ifelse(cmdf[[1]] == cmdf[[2]], "green", "red")
  
  alluvial::alluvial(cmdf[,1:2]
                     , freq = cmdf$Freq
                     , col = cmdf[["color"]]
                     , alpha = 0.5
                     , hide  = cmdf$Freq == 0
                     )
}
plotCM(cm)

```

####Predict zone membership of hex grid
### Use the best model from those above (Biological, nzv and corr removed, Scaled, Outliers Removed)
``` {r Build }
#load("./outputs/USA_Zone_RFModel_Dec4.Rdata")
# bring in grid data with climate attributes
grid.dat.raw <- fread("./inputs/grids/USA_800m_Grid_Climate_new_Normal_1961_1990MSY_2.csv", stringsAsFactors = FALSE,  data.table = FALSE)

#grid.dat <- fread("./inputs/grids/USA_8000m_Grid_Climate_new_Normal_1961_1990MSY.csv", stringsAsFactors = FALSE,  data.table = FALSE)### add in the extra climate variables
Z1 <- grid.dat.raw
Z1$PPT_MJ <- Z1$PPT05 + Z1$PPT06 # MaY/June precip
Z1$PPT_JAS <- Z1$PPT07 + Z1$PPT08 + Z1$PPT09 # July/Aug/Sept precip
Z1$PPT.dormant <- Z1$PPT_at + Z1$PPT_wt # for calculating spring deficit
Z1$CMD.def <- 500 - (Z1$PPT.dormant)# start of growing season deficit original value was 400 but 500 seems better
Z1$CMD.def [Z1$CMD.def < 0] <- 0 #negative values set to zero = no deficit
Z1$CMDMax <- Z1$CMD07
Z1$CMD.grow <- Z1$CMD05 + Z1$CMD06 +Z1$CMD07 +Z1$CMD08 +Z1$CMD09
Z1$CMD.total <- Z1$CMD.def + Z1$CMD
Z1$heatsum <-  Z1 %>% select(starts_with("Tmax")) %>% select(-starts_with("Tmax_")) %>% mutate_all(rdc) %>% mutate_if(zero, setzero) %>% rowSums(na.rm=TRUE)
colnames(Z1)[1:2] <- c("ID1","ID2")
List = c("ID1",  "Latitude", "Longitude", "Elevation")
###reduce variables to list from model selected above
  grid.dat = Z1[,names(Z1) %in% c( List, X1.var)]
  grid.dat <- grid.dat %>%  filter_all(all_vars(. > -9999))
###Predict
grid.dat$Zone <- predict(BGCmodel, newdata = grid.dat[,-c(1:4)])
grid.dat$Zone <-  fct_explicit_na(grid.dat$Zone , na_level = "(None)")
#write.csv(grid.dat, "./outputs/Hexgrid800mPredictedtoSubZone_30vars.csv", row.names = FALSE)
grid.zone <- dplyr::select(grid.dat, ID1, Zone, Latitude, Longitude,Elevation )#"Longitude","Latitude", "Elevation", "Zone")]
write.csv(grid.zone, "./outputs/Hexgrid800mPredictedtoSubZone_2.csv", row.names = FALSE)
```
# Attribute hex grid with Zone call
```{r link to hex polygon layer}
##############link predicted Zones to Polygons and write shape file
hexpoly <- st_read(dsn = "./inputs/hex_shapes", layer = "USA_HexPoly_800m_new")
hexZone <- left_join(hexpoly, grid.zone, by = "ID1")
temp <- select(hexZone, Zone, geometry)

temp2 <- temp
st_precision(temp2) <- 0.5###union polygons within zone
temp2$Zone <-  as.factor(temp2$Zone)
temp2$Zone <- forcats::fct_explicit_na(temp2$Zone,na_level = "(None)")
droplevels(temp2$Zone)
t2 <- temp2 %>%
  group_by(Zone) %>%
  summarise(geometry = sf::st_union(geometry)) %>%
  ungroup()
mapView(t2)
t2 <- st_zm(t2, drop=T, what='ZM')
st_write(t2, dsn = "outputs", layer = "rawSubZoneMap_800m_ranger_30_2", driver = "ESRI Shapefile", update = TRUE)

```
## clean up isolated individual pixels
This process is slow. IS probably better done in QGIS.
In QGIS
Take the multipolygon, separate into polygons (v fast), add geometric attributes (v fast), select polygons < x in size (v fast), eliminate selected polygons merging with smallest neighbour(slow)
Remove crumbs of 3 hex or smaller by longest neibour

``` {r cleanup up crumbs}

###now cleanup and remove crumbs
# t2 <- st_cast(t2, "MULTIPOLYGON") %>% st_cast("POLYGON")
# t2 <- t2 %>%
#   mutate(Area = st_area(.)) %>%
#   mutate(ID = seq_along(Zone))
# unique(t2$Area)
# 
# 
# library(units)
# size <- 2000000
# size <- set_units(size, "m^2")
# tSmall <- t2[t2$Area <= size,]
# t2$Zone <- as.character(t2$Zone)
# 
# require(doParallel)
# coreNum <- as.numeric(detectCores()-1)
# coreNo <- makeCluster(coreNum)
# registerDoParallel(coreNo, cores = coreNum)
# 
# ###loop through each polygon < size, determine intersects, and assign to zone with most edge touching
# ###all the built in functions I found only dealt with holes in the middle of polygons
# new <- foreach(i = 1:length(tSmall$ID), .combine = rbind, .packages = c("foreach","sf")) %dopar% {
#   ID <- tSmall$ID[i]
#   nbrs <- st_intersects(tSmall[i,],t2)[[1]]
#   nbrs <- nbrs[!nbrs %in% ID]
#   if(length(nbrs) == 0){return(NULL)}
#   lines <- st_intersection(t2[ID,],t2[nbrs,])
#   lines <- st_cast(lines)
#   l.len <- st_length(lines)
#   names(l.len) <- lines$Zone.1
#   zn <- names(l.len)[l.len == max(l.len)][1]
#   newDat <- t2[ID,]
#   newDat$Zone <- zn
#   newDat
# }
# 
# stopCluster(coreNo)
# gc()
# temp <- t2[!t2$ID %in% new$ID,]
# t2 <- rbind(temp, new) %>%
#   mutate(Zone = as.factor(Zone))
# 
# ###now have to combine crumbs with existing large polygons
# temp2 <- t2
# st_precision(temp2) <- 0.5
# t2 <- temp2 %>%
#   group_by(Zone) %>%
#   summarise(geometry = sf::st_union(geometry)) %>%
#   ungroup()
# 
# mapview(t2, zcol = "Zone")
# t2 <- st_zm(t2, drop=T, what='ZM')
# st_write(t2, dsn = "outputs", layer = "DeCrumbed_SubZoneMap_800m_ranger30", driver = "ESRI Shapefile", update = TRUE)

```


# Build Subzone models and predict within each Zone
####now build models for each subzone and then predict gridpoints by subzone model
Advice of Tom Hengel is that single model is more mathematically justified

``` {r Build Subzone model and predict}

# # Grid data from previous chunk
# pointDat <- grid.dat
# pointDat <-  pointDat %>% select(Zone, everything()) %>% arrange(Zone)
# 
# ######################################################
# #Training point data from above
# X2 <-  X1.info %>% select(Zone, everything()) %>% arrange(Zone)
# X2$BGC <-  as.factor(X2$BGC)
# Zones <- as.character(unique(X2$Zone))
# Zones <- sort (Zones)
# X2 <- droplevels(X2)
# # X2 <- X2[X2$Zone == "CMA",]
# # Zones <- as.character(unique(X2$Zone))
# # Zones <- sort (Zones)
# SZPred <- foreach(Z = Zones, .combine = rbind) %do% {
#   trainSub <- X2[X2$Zone == Z,]###subset training points to only include selected zone
#    if(length(unique(trainSub$BGC)) >= 2){ ###somtimes there aren't any subzones skip to else
#     trainSub$BGC <- as.factor(trainSub$BGC)
#     trainSub$BGC <- droplevels(trainSub$BGC)
#     trainSub <- removeOutlier(trainSub, alpha = 0.001)
# ###build model for each subzone individually
#     set.seed(123321)
# coreNo <- makePSOCKcluster(detectCores() - 1)
# registerDoParallel(coreNo, cores = detectCores() - 1)
# Cores <- as.numeric(detectCores()-1)
# 
# SZmodel <- train(BGC  ~ ., data = trainSub [-1],
#                      method = "ranger",
#                      trControl = trainControl(method="cv", number = 10, verboseIter = T, classProbs = T, savePredictions = "final"),
#                      #num.trees = 11,
#                      preProcess = c("center", "scale", "YeoJohnson"),#,tuneGrid = tgrid,
#                      importance = "impurity")
# 
#  stopCluster(coreNo)
# gc()
#       # SZmodel <- randomForest(BGC ~ ., data=trainSub [-1], nodesize = 5, do.trace = 10,
#       #                       ntree=101, na.action=na.fail, importance=TRUE, proximity=FALSE)
# 
#    
#     pointSub <- pointDat[pointDat$Zone == Z,] ###subset grid based on previously predicted zones
#     pointSub$BGC<- predict(SZmodel, newdata = pointSub[,-c(1:4),]) ### predict subzones
#     out <- pointSub[,c("ID1", "Latitude","Longitude","Zone", "BGC")]
#     out
#   }else{ ##if only one subzone, plot
#     pointSub <- pointDat[pointDat$Zone == Z,]
#     #pointSub$Zone <- droplevels(pointSub$Zone)
#     pointSub$BGC <- pointSub$Zone
#     out <- pointSub[,c("ID1", "Latitude","Longitude","Zone", "BGC")]
#     out
#   }
#   
# }
# 
# grid.sbz <- dplyr::select(SZPred, ID1, BGC, Zone)#"Longitude","Latitude", "Elevation", "Zone")]
# grid.sbz <- droplevels(grid.sbz)
# table(grid.sbz$BGC)
# grid.dat.pred <- left_join(grid.dat, grid.sbz, by = c("ID1", "Zone"))
# grid.dat.pred <- grid.dat.pred %>% select(ID1, Zone, BGC, everything())
# write.csv(grid.dat, "./outputs/Hexgrid800mPredictedtoSubZone_ranger.csv", row.names = FALSE)
# ```
# # Attribute hexpolygons with subzone predictions 
# ```{r link subzone predictions to hex polygons}
# ###link to hex polygons
# hexZone2 <- left_join(hexZone, grid.sbz, by = "ID1")
# #temp <- select(hexZone, Zone, geometry)
# temp <- hexZone2
# temp$BGC <-  fct_explicit_na(temp$BGC , na_level = "(None)")
# table(temp$BGC)
# 
# temp2 <- temp
# st_precision(temp2) <- 0.5###union polygons within zsubone
# t3 <- temp2 %>%
#   group_by(BGC) %>%
#   summarise(geometry = sf::st_union(geometry)) %>%
#   ungroup()
# 
# mapview(t3)
# t3 <- st_zm(t3, drop=T, what='ZM')
# st_write(t3, dsn = "outputs", layer = "SubzoneRaw800m_ranger_30", driver = "ESRI Shapefile", update = TRUE)


```

## This should probably be done from the spatial file since it will have be simplified with decrumbing

```{r cLHS of trainingpoints for WNA}
#select a subset of training points by BGC from the final surface for full model build
# library(clhs)
# #BGC = "ICHxwz"
# #countZone <- points %>% count(Zone)
# countSubzone <- grid.dat.pred %>% count(BGC)
# 
# rownames(grid.dat.pred) <- grid.dat.pred[,1]
# countSubzone$logn <- log(countSubzone$n, 10)
# countSubzone$rs <- as.integer (rescale(countSubzone$logn, to = c(500, 1200), from = range(countSubzone$logn, na.rm = TRUE, finite = TRUE)))
# countSubzone$sample <- ifelse(countSubzone$rs > countSubzone$n, countSubzone$n, countSubzone$rs )
# write.csv (countSubzone, "./outputs/USA_pts_per_BECLHC2.csv")
# 
# allUnits <- unique(grid.dat.pred$BGC)
# #grid.dat.pred$ID1 <- row.names(grid.dat.pred)
# set.seed(123321)
# coreNo <- makePSOCKcluster(detectCores() - 1)
# registerDoParallel(coreNo, cores = detectCores() - 1)
# Cores <- as.numeric(detectCores()-1)
# 
# BGC = "BGdh_OR"
# LHCtraining <- foreach(BGC = allUnits, .combine = rbind, .packages = c("clhs")) %dopar% {
# temp <- grid.dat.pred[(grid.dat.pred$BGC %in% BGC),]
# temp_names <- temp[1:6]
# Num <- countSubzone$sample[(countSubzone$BGC %in% BGC)]
#   samples <- clhs(temp[,-c(1:6)], 
#                 size = Num,           # Test a range of sample sizes
#                 iter = 10000,        # Arbitrarily large number of iterations of optimization procedure. Default=10,000 but a larger number may be used
#                 progress = TRUE, 
#                 simple = FALSE)
# 
# cLHS <- samples$sampled_data
# cLHS$ID1 <- row.names (cLHS)
# cLHS_Points <- merge (temp[,c(1:5)], cLHS, by = "ID1")
# cLHS_Points
# }
#  stopCluster(coreNo)
#  gc()
# 
# write.csv(LHCtraining, "./outputs/USA_Training_LHC_w_data.csv", row.names = FALSE)
# LHCtraining2 <- LHCtraining [1:3] 
# rownames(grid.dat.raw) <- grid.dat.raw[,1]
# LHCtraining2 <- left_join(LHCtraining2, grid.dat.raw, by = "ID1")
# write.csv(LHCtraining2, "./outputs/USA_LHS_all_dat.csv", row.names = FALSE)
# USA_LHS <- dplyr::select(LHCtraining2, ID1, BGC, Longitude, Latitude, Elevation)
# write.csv(USA_LHS, "./outputs/USA_LHS_for_ClimateNA.csv", row.names = FALSE)
#X1 <- LHCtraining #feed these back into building a new rF model above
```
