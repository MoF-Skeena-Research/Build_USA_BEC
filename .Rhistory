X1$CMD.def [X1$CMD.def < 0] <- 0 #negative values set to zero = no deficit
X1$CMDMax <- X1$CMD07
X1$CMD.total <- X1$CMD.def + X1$CMD
####New 16 Variable Set
VarList = c("TD",	"Eref",	"AHM",	"Tmax07",	"CMD.total",	"Tmax_sp","Tmin_sm","CMD_sp","PAS",
"PPT_sp","PPT06","CMD.def","DD5_sp","Tmin_at","bFFP","MCMT")
List = c("ID1", "ID2", "Latitude", "Longitude", "Elevation")
X1 = X1[,names(X1) %in% c(List,VarList)]
colnames(X1)[1:2] <- c("PlotNo","BGC")
X1 <- X1[,-c(1,3:5)]
trainClean <- X1
View(trainClean)
X1 <- fread("./inputs/training_pts/US_TrainingPointsAll_18Oct2019_Normal_1961_1990MSY.csv",  stringsAsFactors = FALSE,data.table = FALSE)
View(X1)
colnames(X1) [2] <- "ID2"
View(X1)
X1$PPT_MJ <- X1$PPT05 + X1$PPT06 # MaY/June precip
X1$PPT_JAS <- X1$PPT07 + X1$PPT08 + X1$PPT09 # July/Aug/Sept precip
X1$PPT.dormant <- X1$PPT_at + X1$PPT_wt # for calculating spring deficit
X1$CMD.def <- 500 - (X1$PPT.dormant)# start of growing season deficit original value was 400 but 500 seems better
X1$CMD.def [X1$CMD.def < 0] <- 0 #negative values set to zero = no deficit
X1$CMDMax <- X1$CMD07
X1$CMD.total <- X1$CMD.def + X1$CMD
####New 16 Variable Set
VarList = c("TD",	"Eref",	"AHM",	"Tmax07",	"CMD.total",	"Tmax_sp","Tmin_sm","CMD_sp","PAS",
"PPT_sp","PPT06","CMD.def","DD5_sp","Tmin_at","bFFP","MCMT")
List = c("ID1", "ID2", "Latitude", "Longitude", "Elevation")
X1 = X1[,names(X1) %in% c(List,VarList)]
View(X1)
colnames(X1) [1,2] <- c("ID1, ID2")
X1 <- fread("./inputs/training_pts/US_TrainingPointsAll_18Oct2019_Normal_1961_1990MSY.csv",  stringsAsFactors = FALSE,data.table = FALSE)
colnames(X1) [1,2] <- c("ID1, ID2")
colnames(X1) [1:2] <- c("ID1, ID2")
View(X1)
X1 <- fread("./inputs/training_pts/US_TrainingPointsAll_18Oct2019_Normal_1961_1990MSY.csv",  stringsAsFactors = FALSE,data.table = FALSE)
colnames(X1) [,1:2] <- c("ID1, ID2")
colnames(X1) [1:2] <- c("ID1, ID2")
View(X1)
X1 <- fread("./inputs/training_pts/US_TrainingPointsAll_18Oct2019_Normal_1961_1990MSY.csv",  stringsAsFactors = FALSE,data.table = FALSE)
colnames(X1) [2] <- "ID2"
View(X1)
X1 <- fread("./inputs/training_pts/US_TrainingPointsAll_18Oct2019_Normal_1961_1990MSY.csv",  stringsAsFactors = FALSE,data.table = FALSE)
colnames(X1) [2] <- "ID2"
colnames(X1) [1] <- "ID1"
X1$PPT_MJ <- X1$PPT05 + X1$PPT06 # MaY/June precip
X1$PPT_JAS <- X1$PPT07 + X1$PPT08 + X1$PPT09 # July/Aug/Sept precip
X1$PPT.dormant <- X1$PPT_at + X1$PPT_wt # for calculating spring deficit
X1$CMD.def <- 500 - (X1$PPT.dormant)# start of growing season deficit original value was 400 but 500 seems better
X1$CMD.def [X1$CMD.def < 0] <- 0 #negative values set to zero = no deficit
X1$CMDMax <- X1$CMD07
X1$CMD.total <- X1$CMD.def + X1$CMD
####New 16 Variable Set
VarList = c("TD",	"Eref",	"AHM",	"Tmax07",	"CMD.total",	"Tmax_sp","Tmin_sm","CMD_sp","PAS",
"PPT_sp","PPT06","CMD.def","DD5_sp","Tmin_at","bFFP","MCMT")
List = c("ID1", "ID2", "Latitude", "Longitude", "Elevation")
X1 = X1[,names(X1) %in% c(List,VarList)]
colnames(X1)[1:2] <- c("PlotNo","BGC")
X1 <- X1[,-c(1,3:5)]
trainClean <- X1
library(UBL) ###contains smote function
X1$BGC <- gsub("[[:lower:]]|[[:space:]]|[[:digit:]]", "", X1$BGC) ###for zone model
X1 <- X1[X1$BGC != "",]
X1$BGC <- as.factor(X1$BGC)
###This function uses mahalanobis distance to remove outlying training points below specified percentile
removeOutlier <- function(dat, alpha = 0.001){
out <- foreach(curr = unique(as.character(dat$BGC)), .combine = rbind) %do% {
temp <- dat[dat$BGC == curr,]
md <- tryCatch(mahalanobis(temp[,-1],center = colMeans(temp[,-1]), cov = cov(temp[,-1])),
error = function(e) e
)
if(!inherits(md,"error")){
ctf <- qchisq(1-alpha, df = ncol(temp)-1)
outl <- which(md > ctf)
cat("Removing", length(outl), "outliers from",curr, "; ")
if(length(outl) > 0){
temp <- temp[-outl,]
}
}
temp
}
return(out)
}
X1.sub <- removeOutlier(X1, alpha = 0.01)
X1 <- X1.sub
# ##for equal numbers of training pnts
# tpNum <- 750 ##number of desired training points per unit
# mList <- list()
# for(BGC in unique(X1$BGC)){##Calculate multiplier
#   num <- length(X1$BGC[X1$BGC == BGC])
#   mNum <- tpNum/num
#   mList[[BGC]] <- mNum
# }
###min number per unit
tpMin <- 200
mList <- list()
for(BGC in unique(X1$BGC)){##Calculate multiplier
num <- length(X1$BGC[X1$BGC == BGC])
if(num < tpMin){
mNum <- tpMin/num
}else{
mNum <- 1
}
mList[[BGC]] <- mNum
}
###SMOTE!
X1.smote <- SmoteClassif(BGC ~ ., X1, C.perc = mList, k= 5 , repl = TRUE, dist = "Euclidean")
table(X1.smote$BGC)
BGCmodel <- randomForest(BGC ~ ., data=X1.smote, nodesize = 5, do.trace = 10,
ntree=101, na.action=na.fail, importance=TRUE, proximity=FALSE)
save(BGCmodel,file= "./rF_models/USA_Zone_RFModel_16Var.rFModel")
load("USA_Zone_RFModel_16Var.rFModel")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(sp)
library(sf)
library(raster)
library(rgeos)
library(rgbif)
library(viridis)
library(gridExtra)
library(rasterVis)
library(mapview)
library(foreach)
library(randomForest)
library(data.table)
library(units)
require (tictoc)
rm(list=ls())
options(stringsAsFactors = FALSE)
X1 <- fread("./inputs/training_pts/US_TrainingPointsAll_18Oct2019_Normal_1961_1990MSY.csv",  stringsAsFactors = FALSE,data.table = FALSE)
colnames(X1) [2] <- "ID2"
colnames(X1) [1] <- "ID1"
X1$PPT_MJ <- X1$PPT05 + X1$PPT06 # MaY/June precip
X1$PPT_JAS <- X1$PPT07 + X1$PPT08 + X1$PPT09 # July/Aug/Sept precip
X1$PPT.dormant <- X1$PPT_at + X1$PPT_wt # for calculating spring deficit
X1$CMD.def <- 500 - (X1$PPT.dormant)# start of growing season deficit original value was 400 but 500 seems better
X1$CMD.def [X1$CMD.def < 0] <- 0 #negative values set to zero = no deficit
X1$CMDMax <- X1$CMD07
X1$CMD.total <- X1$CMD.def + X1$CMD
####New 16 Variable Set
VarList = c("TD",	"Eref",	"AHM",	"Tmax07",	"CMD.total",	"Tmax_sp","Tmin_sm","CMD_sp","PAS",
"PPT_sp","PPT06","CMD.def","DD5_sp","Tmin_at","bFFP","MCMT")
List = c("ID1", "ID2", "Latitude", "Longitude", "Elevation")
X1 = X1[,names(X1) %in% c(List,VarList)]
colnames(X1)[1:2] <- c("PlotNo","BGC")
X1 <- X1[,-c(1,3:5)]
trainClean <- X1
library(UBL) ###contains smote function
X1$BGC <- gsub("[[:lower:]]|[[:space:]]|[[:digit:]]", "", X1$BGC) ###for zone model
X1 <- X1[X1$BGC != "",]
X1$BGC <- as.factor(X1$BGC)
###This function uses mahalanobis distance to remove outlying training points below specified percentile
removeOutlier <- function(dat, alpha = 0.001){
out <- foreach(curr = unique(as.character(dat$BGC)), .combine = rbind) %do% {
temp <- dat[dat$BGC == curr,]
md <- tryCatch(mahalanobis(temp[,-1],center = colMeans(temp[,-1]), cov = cov(temp[,-1])),
error = function(e) e
)
if(!inherits(md,"error")){
ctf <- qchisq(1-alpha, df = ncol(temp)-1)
outl <- which(md > ctf)
cat("Removing", length(outl), "outliers from",curr, "; ")
if(length(outl) > 0){
temp <- temp[-outl,]
}
}
temp
}
return(out)
}
X1.sub <- removeOutlier(X1, alpha = 0.01)
X1 <- X1.sub
# ##for equal numbers of training pnts
# tpNum <- 750 ##number of desired training points per unit
# mList <- list()
# for(BGC in unique(X1$BGC)){##Calculate multiplier
#   num <- length(X1$BGC[X1$BGC == BGC])
#   mNum <- tpNum/num
#   mList[[BGC]] <- mNum
# }
###min number per unit
tpMin <- 200
mList <- list()
for(BGC in unique(X1$BGC)){##Calculate multiplier
num <- length(X1$BGC[X1$BGC == BGC])
if(num < tpMin){
mNum <- tpMin/num
}else{
mNum <- 1
}
mList[[BGC]] <- mNum
}
###SMOTE!
X1.smote <- SmoteClassif(BGC ~ ., X1, C.perc = mList, k= 5 , repl = TRUE, dist = "Euclidean")
table(X1.smote$BGC)
View(X1.smote)
BGCmodel <- randomForest(BGC ~ ., data=X1.smote, nodesize = 5, do.trace = 10,
ntree=101, na.action=na.fail, importance=TRUE, proximity=FALSE)
save(BGCmodel,file= "./rF_models/USA_Zone_RFModel_16Var_25Oct.rFModel")
load("USA_Zone_RFModel_16Var.rFModel")
pointDat <- fread("./inputs/grids/USA_800m_Grid_Climate_Normal_1961_1990MSY.csv",
stringsAsFactors = FALSE,  data.table = FALSE)
X1 <- pointDat
X1$PPT_MJ <- X1$PPT05 + X1$PPT06 # MaY/June precip
X1$PPT_JAS <- X1$PPT07 + X1$PPT08 + X1$PPT09 # July/Aug/Sept precip
X1$PPT.dormant <- X1$PPT_at + X1$PPT_wt # for calculating spring deficit
X1$CMD.def <- 500 - (X1$PPT.dormant)# start of growing season deficit original value was 400 but 500 seems better
X1$CMD.def [X1$CMD.def < 0] <- 0 #negative values set to zero = no deficit
X1$CMDMax <- X1$CMD07
X1$CMD.total <- X1$CMD.def + X1$CMD
colnames(X1)[1:2] <- c("ID1","ID2")
####New 16 Variable Set
VarList = c("TD",	"Eref",	"AHM",	"Tmax07",	"CMD.total",	"Tmax_sp","Tmin_sm","CMD_sp","PAS",
"PPT_sp","PPT06","CMD.def","DD5_sp","Tmin_at","bFFP","MCMT")
List = c("ID1", "ID2", "Latitude", "Longitude", "Elevation")
X1 = X1[,names(X1) %in% c(List,VarList)]
pointDat <- X1
###Predict
tic()
pointDat$Zone <- predict(BGCmodel, newdata = pointDat[,-c(1:5)])
testpnts <- pointDat[,c("ID1","Zone")]
#testpnts$ID1 <- as.character(testpnts$ID1)
toc()
hexPoly <- st_read(dsn = "./inputs/spatial_poly", layer = "USA_HexPoly_800m")
colnames (hexPoly)[1] <- "ID1"
hexPoly <- unlist(hexPoly)
hexPoly <- st_read(dsn = "./inputs/spatial_poly", layer = "USA_HexPoly_800m")
colnames (hexPoly)[1] <- "ID1"
hexPoly <- unlist(hexPoly)
# testpnts <- testpnts[testpnts$Latitude > 43.407 & testpnts$Latitude < 46.5 & testpnts$Longitude < -111.79 & testpnts$Longitude > -115.412,]##subset to small area for example
tic()
temp <- merge(hxPnts, testpnts, by = "ID1", all = T)
tic()
temp <- merge(hexPoly, testpnts, by = "ID1", all = T)
View(testpnts)
View(testpnts)
str(testpnts)
str(hexPoly)
hexPoly <- st_read(dsn = "./inputs/spatial_poly", layer = "USA_HexPoly_800m")## make sure that the polygons have a single variable named "ID1"
hexPoly <- st_read(dsn = "./inputs/spatial_poly", layer = "USA_HexPoly_800m")## make sure that the polygons have a single variable named "ID1"
colnames (hexPoly)[1] <- "ID1"
head(hexPoly)
str(hexPoly)
hexPoly2 <- unlist(hexPoly)
hexPoly2
tic()
temp <- merge(hexPoly, testpnts, by = "ID1", all = T)
toc()
st_precision(temp2) <- 0.5###union polygons within zone
# tic()
# temp <- join(hexPoly, testpnts, by = "ID1") ## problem with non-atomic vectors for this function
# toc()
temp <- temp[,-1]
temp2 <- temp %>% drop_na(Zone)
st_precision(temp2) <- 0.5###union polygons within zone
t1 <- temp2 %>%
group_by(Zone) %>%
summarise(geometry = sf::st_union(geometry)) %>%
ungroup()
##mapview(t1)
###now cleanup and remove crumbs
t2 <- st_cast(t1, "MULTIPOLYGON") %>% st_cast("POLYGON")
t2 <- t2 %>%
mutate(Area = st_area(.)) %>%
mutate(ID = seq_along(Zone))
size <- 2 # 2.5 100## clean up polygons < size
size <- set_units(size, "km^2")
tSmall <- t2[t2$Area < size,]
t2$Zone <- as.character(t2$Zone)
#t2 <- t2[!(is.na(t2$Zone)),]
require(doParallel)
coreNum <- as.numeric(detectCores()-1)
coreNo <- makeCluster(coreNum)
registerDoParallel(coreNo, cores = coreNum)
###loop through each polygon < size, determine intersects, and assign to zone with most edge touching
### will fail if there are no polygons < size
new <- foreach(i = 1:length(tSmall$ID), .combine = rbind, .packages = c("foreach","sf")) %dopar% {
ID <- tSmall$ID[i]
nbrs <- st_intersects(tSmall[i,],t2)[[1]]
nbrs <- nbrs[!nbrs %in% ID]
if(length(nbrs) == 0){return(NULL)}
lines <- st_intersection(t2[ID,],t2[nbrs,])
lines <- st_cast(lines)
l.len <- st_length(lines)
names(l.len) <- lines$Zone.1
zn <- names(l.len)[l.len == max(l.len)][1]
newDat <- t2[ID,]
newDat$Zone <- zn
newDat
}
stopCluster(coreNo)
temp <- t2[!t2$ID %in% new$ID,]
t2 <- rbind(temp, new) %>%
mutate(Zone = as.factor(Zone))
###now have to combine re-zoned crumbs with existing large polygons
temp2 <- t2
st_precision(temp2) <- 0.5
t1 <- temp2 %>%
group_by(Zone) %>%
summarise(geometry = sf::st_union(geometry)) %>%
ungroup()
st_write(t1, dsn = "FinalMaps", layer = "USA_Zonea", driver = "ESRI Shapefile", update = TRUE, delete_layer = TRUE)
mapview(t1, zcol = "Zone")
View(t2)
View(t1)
hexPoly <- st_read(dsn = "./inputs/spatial_poly", layer = "USA_HexPoly_800m")## make sure that the polygons have a single variable named "ID1"
hexPoly <- st_read(dsn = "./inputs/spatial_poly", layer = "USA_HexPoly_800m")## make sure that the polygons have a single variable named "ID1"
colnames (hexPoly)[1] <- "ID1"
#hexPoly2 <- unlist(hexPoly)
# testpnts <- testpnts[testpnts$Latitude > 43.407 & testpnts$Latitude < 46.5 & testpnts$Longitude < -111.79 & testpnts$Longitude > -115.412,]##subset to small area for example
tic()
temp <- merge(hexPoly, testpnts, by = "ID1", all = T)
toc()
# tic()
# temp <- join(hexPoly, testpnts, by = "ID1") ## problem with non-atomic vectors for this function
# toc()
temp <- temp[,-1]
temp2 <- temp %>% drop_na(Zone)
#temp2 <- temp2[!(is.na(temp2$Zone)),]
st_precision(temp2) <- 0.5###union polygons within zone
t1 <- temp2 %>%
group_by(Zone) %>%
summarise(geometry = sf::st_union(geometry)) %>%
ungroup()
##mapview(t1)
###now cleanup and remove crumbs
t2 <- st_cast(t1, "MULTIPOLYGON") %>% st_cast("POLYGON")
t2 <- t2 %>%
mutate(Area = st_area(.)) %>%
mutate(ID = seq_along(Zone))
size <- 2 # 2.5 100## clean up polygons < size
size <- set_units(size, "km^2")
tSmall <- t2[t2$Area < size,]
t2$Zone <- as.character(t2$Zone)
#t2 <- t2[!(is.na(t2$Zone)),]
require(doParallel)
coreNum <- as.numeric(detectCores()-1)
coreNo <- makeCluster(coreNum)
registerDoParallel(coreNo, cores = coreNum)
###loop through each polygon < size, determine intersects, and assign to zone with most edge touching
### will fail if there are no polygons < size
new <- foreach(i = 1:length(tSmall$ID), .combine = rbind, .packages = c("foreach","sf")) %dopar% {
ID <- tSmall$ID[i]
nbrs <- st_intersects(tSmall[i,],t2)[[1]]
nbrs <- nbrs[!nbrs %in% ID]
if(length(nbrs) == 0){return(NULL)}
lines <- st_intersection(t2[ID,],t2[nbrs,])
lines <- st_cast(lines)
l.len <- st_length(lines)
names(l.len) <- lines$Zone.1
zn <- names(l.len)[l.len == max(l.len)][1]
newDat <- t2[ID,]
newDat$Zone <- zn
newDat
}
stopCluster(coreNo)
temp3 <- t2[!t2$ID %in% new$ID,]
t3 <- rbind(temp3, new) %>%
mutate(Zone = as.factor(Zone))
###now have to combine re-zoned crumbs with existing large polygons
temp4 <- t3
st_precision(temp4) <- 0.5
t4 <- temp4 %>%
group_by(Zone) %>%
summarise(geometry = sf::st_union(geometry)) %>%
ungroup()
st_write(t4, dsn = "FinalMaps", layer = "USA_Zonea", driver = "ESRI Shapefile", update = TRUE, delete_layer = TRUE)
mapview(t4, zcol = "Zone")
st_write(t1, dsn = "FinalMaps", layer = "USA_Zone_unsimplified", driver = "ESRI Shapefile", update = TRUE, delete_layer = TRUE)
mapview(t1)
str(hexPoly)
hexPoly2 <- hexPoly %>% st_set_crs(3005)
str(testpnts)
head(pointDat)
X1b <- X1 [!(X1$MCMT == -9999.0),]
pointDat <- X1b
###Predict
tic()
pointDat$Zone <- predict(BGCmodel, newdata = pointDat[,-c(1:5)])
pointDat$Zone <- predict(BGCmodel, newdata = pointDat[,-c(1:5)])
testpnts <- pointDat[,c("ID1","Zone")]
#testpnts$ID1 <- as.character(testpnts$ID1)
toc()
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(sp)
library(sf)
library(raster)
library(rgeos)
library(rgbif)
library(viridis)
library(gridExtra)
library(rasterVis)
library(mapview)
library(foreach)
library(randomForest)
library(data.table)
library(units)
require (tictoc)
rm(list=ls())
options(stringsAsFactors = FALSE)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(dplyr)
library(rgdal)
library(sp)
library(raster)
library(rgeos)
library(maptools)
library(magrittr)
library(tibble)
library(tidyr)
library(sf)
library(tcltk)
library(foreach)
library(httr)
library(jsonlite)
library(randomForest)
library(data.table)
require(survey)
require(fasterize)
require(tibble)
require(maptools)
require(tictoc)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(dplyr)
library(rgdal)
library(sp)
library(raster)
library(rgeos)
library(maptools)
library(magrittr)
library(tibble)
library(tidyr)
library(sf)
library(tcltk)
library(foreach)
library(httr)
library(jsonlite)
library(randomForest)
library(data.table)
require(survey)
require(fasterize)
require(tibble)
require(maptools)
require(tictoc)
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(dplyr)
library(rgdal)
library(sp)
library(raster)
library(rgeos)
library(maptools)
library(magrittr)
library(tibble)
library(tidyr)
library(sf)
library(tcltk)
library(foreach)
library(httr)
library(jsonlite)
library(randomForest)
library(data.table)
require(survey)
require(fasterize)
require(tibble)
require(maptools)
require(tictoc)
source("_Function_make_HexGrid.R")
CRS.albers <- CRS ("+proj=aea +lat_1=50 +lat_2=58.5 +lat_0=45 +lon_0=-126 +x_0=1000000 +y_0=0 +datum=NAD83 +units=m +no_defs")
CRS.NAD83 <- CRS("+init=epsg:4269")
### Read in the combined state boundary files for WNA
USA <- readOGR(dsn = "./inputs/spatial_poly/", layer = "USA_States")
source("_Function_make_HexGrid.R")
CRS.albers <- CRS ("+proj=aea +lat_1=50 +lat_2=58.5 +lat_0=45 +lon_0=-126 +x_0=1000000 +y_0=0 +datum=NAD83 +units=m +no_defs")
CRS.NAD83 <- CRS("+init=epsg:4269")
### Read in the combined state boundary files for WNA
USA <- readOGR(dsn = "./inputs/spatial_poly", layer = "USA_States")
USA <- spTransform(USA, CRS.albers)
USA_buff <- gBuffer(USA, byid=FALSE, width = 1) ##expand size of USA to capture boundaries
size <- 800
USA_BGC_pts <- make_grid(USA_buff, cell_diameter = size, clip = FALSE)
#convert to SpatialPointsDataFrame and write shape file
p <- data.frame (ID1 = 1 :length(USA_BGC_pts))
USA_BGC_pts2 <- SpatialPointsDataFrame(USA_BGC_pts, data = p)
writeOGR (obj = USA_BGC_pts2, dsn = "./inputs/spatial_poly/USA_HexGrid_800m_new.shp",
layer = "GridPoints", driver = "ESRI Shapefile", overwrite_layer = TRUE)
#create CSV output for ClimateBC
p2 <- spTransform(USA_BGC_pts, CRS.NAD83) # to lat long in NAD83
coords <- p2@coords
coords <- as.data.frame(coords)
WNA_DEM <- raster("./inputs/DEM/WNA_DEM_clipped.tif")
projection(WNA_DEM) <- CRS.NAD83
coords$elev <- raster::extract(WNA_DEM,p2)
colnames(coords) <- c("longitude","latitude","elevation")
coords <- as_tibble(rownames_to_column(coords, var = "ID1"))
#overlay of points over administrative boundaries
USA$State <- as.character (USA$State)
pnt_state <- over(USA_BGC_pts, USA[,"State"],  minDimension = 1, returnList = FALSE)
pnt_state <- as_tibble(rownames_to_column(pnt_state, var = "ID1"))
pts.info <- left_join(pnt_state, coords,  by = "ID1")
pts.info2 <- pts.info %>% dplyr::select(ID1, State, latitude, longitude, elevation) #order for ClimateWNA
write.csv (pts.info2,  "./inputs/grids/USA_800m_Grid_Climate_new.csv", row.names = FALSE)
USA_BGC_hex <- make_hex (USA_buff, cell_diameter = size, clip = FALSE)
#convert to SpatialPolygonsDataFrame
o <- data.frame (ID1 = 1 :length(USA_BGC_hex))
USA_BGC_hex2 <- SpatialPolygonsDataFrame(USA_BGC_hex, data = o)
writeOGR (obj = USA_BGC_hex2, dsn = "./inputs/spatial_poly/USA_HexPoly_800m_new.shp", layer = "HexPolys", driver = "ESRI Shapefile", overwrite_layer = TRUE)
